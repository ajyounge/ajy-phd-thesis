%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Related Research}
\label{chap:related}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

In order to accurately depict the research presented in this article, the topics within Virtualization,  Cloud computing, and High Performance Computing are reviewed in detail. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Virtualization}
\label{sec:virtualization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


Virtualization is a way to abstract the hardware and system resources from a operating system.  This is typically performed  within a Cloud environment across a large set of servers using a Hypervisor or Virtual Machine Monitor (VMM) which lies in between the hardware and the Operating System (OS). From here, one or more virtualized OSs can be started concurrently as seen in Figure \ref{F:1}, leading to one of the key advantages of Cloud computing.  This, along with the advent of multi-core processing capabilities, allows for a consolidation of resources within any data center.  It is the Cloud's job to exploit this capability to its maximum potential while still maintaining a given QoS. It should be noted that virtualization is not specific to Cloud computing. IBM originally pioneered the concept in the 1960's with the M44/44X systems.  It has only recently been reintroduced for general use on x86 platforms. 


 \FIGURE{htb}
  {images/Slide2.pdf}
  {0.8}
  {Virtual Machine Abstraction}
  {F:vmm}
 

However, virtualization is in the most general form, just another form of abstraction. As such, there are in fact many levels to virtualization that exist \cite{hwang2013distributed}. 


\begin{itemize}

\item \textbf{ISA - }
Virtualization can start from the instruction set architecture (ISA) level, where by an entire processor instruction set is emulated and provided.  This may be useful for running sotfware or services developed for one instruction set (say MIPS) but is needed to run on Intel x86 hardware. ISA level virtualization is usually emulated through an interpreter which translates source instructions to target instructions, however this can often be extremely inefficient.  Dynamic binary translation can help aid in efficiency by translating bocks of source instructions to target instructions, however this still can be limiting.   

\item \textbf{Hardware - }
One of the most important technologies in cloud computing is hardware level virtualization\cite{Barham2003, ESX}. Hardware virtualization, in its most pure form, refers to the process of creating virtual abstraction to hardware platforms, operating systems, or software resources. This enables the creation of 1 or more virtual machines (VMs) that are run concurrently on the same operating environment, be it hardware or some higher software. Here, a virtual hardware environment is generating for a VM, including providing virtual processors, memory, and I/O devices, allowing for a multiplexing of VMs to exist, as depicted in Figure \ref{F:vmm}.  This layer also manages the physical hardware on behalf of a host OS as well as for guests. While the most common type of hardware virtualization is with the Xen Virtual Machine Monitor \cite{Barham2003}, this method has further separated into type 1 and type 2 hypervisors, as detailed further in Section \ref{s:hypervisors}.  

\item \textbf{Operating System - }
Moving up the latter of implementation with virtualization, we find OS level virtualization, where multiplexing happens at the level of the OS, rather than the hardware. Usually, this refers to isolating a filesystem and associated process and runtime effects in a single "chroot" or "container" environment at the user level, with kernel level operations being handled by a single OS kernel. This containerization allows for multiple user environments to exist concurrently without the complexity of hardware or ISA level virtualization.  Containers are also describe in the next section in more detail.

\item \textbf{Library (API) - }
Continuing, we move up to library or API level virtualzation. Here, some API is separated and a provided to a user or programming environment, and the communication between this API and the back-end library with the computation is virtualized. This can give the effect that resources are local, when really they are remote and abstract. This is how GPUs are "virtualized" with tools such as rCUDA \cite{duato2010rcuda, duato2011enabling} and vCUDA \cite{shi2012vcuda}. This method's performance often can be very dependent on the underlying communications mechanisms, as well as complete virtualization of the whole API (which may be difficult).  API virtualization also exists for entire OS libraries to translate between differing OS, without actual containerization.   

\item \textbf{Process}
Virtualization can also take the form at the process or user level, which can then be used to deliver a specialzied or high level language that is the same across several different OSs. This is how the Java Virtual Machine (JVM) and Microsoft's .NET platform work. This type of application predominantly falls outside of the scope of this dissertation. 


\end{itemize}



%%%%%%%%%---------------------------------------------------------
\subsection{Hypervisors and Containers}
\label{s:hypervisors}
%%%%%%%%%---------------------------------------------------------

While there are many types of virtualization, this dissertation predominately focuses on hardware and OS level virtualization only. With hardware virtualization, a Virtual Machine Monitor, or hypervisor, is used. 

\TODO{Write definition of a hypervisor}

 \FIGURE{htb}
  {images/hypervisortypes.jpg}
  {1.0}
  {Hypervisors and Containers}
  {F:hypervisors}

Hardware virtualization can actually be dissected in more detail to two different types of hypervisors, and they can be directly compared to containers as seen in Figure \ref{F:hypervisors}. With a Type 1 virtuallization system, the hypervisor or VMM sits directly on the bare-metal hardware, below the OS. These native hypervisors provide direct control of the underlying hardware, and are controlled and operated usually through the use of a privileged VM.  One example of a type 1 hypervisor is the Xen Virtual Machine Monitor \cite{Barham2003}, which uses its VMM as well as a privileged Linux OS, called Dom0, to create and manage other user VMs, or running DomU instances. The VMM in this place provides the necessary hardware abstraction of CPU, memory, and some I/O aspects, leaving the control aspects of the other DomUs to Dom0.  With a type 1 hypervisor, all virtualization functions are kept separate from control and OS functionaity, effectively making a cleaner design. 

Type 2 hypervisors utilize a different, and sometimes more convoluted design. With a type 2 hypervisor, there is a "host" OS that, like with native OSs, sits directly atop hardware. This OS is just like any normal native OS. However, the OS itself can abstract its own hardware, and provide and manage a VM, effectively as an OS process.  In this case, the hypervisor providing the abstraction is effectively hosted within, atop, or as a module part of a given OS.  There are many different type 2 hypervisors, the most common of which is the Linux Kernel Virtual Machine (KVM) \cite{kivity2007kvm}. KVM is often used in conjunction with QEMU, a ISA level hypervisor to provide some basic ISA level virtualization and emulation capabilities. KVM is simply provided as a Linux kernel module within a given host, and guest VMs are run as a single process on the host OS. 
 
Both types of hypervisors are very distinct from OS level virtualization, also known as containers. With containers, there is a single OS, however instead of direct hardware abstraction,a single kernel is used to simultaniously run multiple user-space instances in a jailed-root environment. These environments may look and feel like a separate machine, but in fact are not. Often times the kernel itself provides resource management tools to help control resource utilization and allocations. Linux containers \cite{lxc}


 While there are many forms of virtualization, the focus here is primarily on hardware and OS level virtualization, instead of higher level and API based virtualization techniques.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cloud Computing}
\label{sec:cloudcomputing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Cloud computing is one of the most explosively expanding technologies in the computing industry today. However it is important to understand where it came from, in order to figure out where it will be heading in the future.  While there is no clear cut evolutionary path to Clouds, many believe the concepts originate from two specific areas: Grid Computing and Web 2.0.

Grid computing \cite{foster2001a, foster2002b}, in its practical form, represents the concept of connecting two or more spatially and administratively diverse clusters or supercomputers together in a federating manner.  The term ``the Grid" was coined in the mid 1990's to represent a large distributed systems infrastructure for advanced scientific and engineering computing problems. Grids aim to enable applications to harness the full potential of resources through coordinated and controlled resource sharing by scalable virtual organizations.  While not all of these concepts carry over to the Cloud, the control, federation, and dynamic sharing of resources is conceptually the same as in the Grid.  This is outlined by \cite{foster2008cca}, as Grids and Clouds are compared at an abstract level and many concepts are remarkably similar.  From a scientific perspective, the goals of Clouds and Grids are also similar.  Both systems attempt to provide large amounts of computing power by leveraging a multitude of sites running diverse applications concurrently in symphony.  The only significant differences between Grids and Clouds exist in the implementation details, and the reproductions of them, as outlined later in this section.

The other major component, Web 2.0, is also a relatively new concept in the history of Computer Science.  The term Web 2.0 was originally coined in 1999 in a futuristic prediction by Dracy DiNucci \cite{dinucci1999fragmented}: ``The Web we know now, which loads into a browser window in essentially static screenfulls, is only an embryo of the Web to come. The first glimmerings of Web 2.0 are beginning to appear, and we are just starting to see how that embryo might develop. The Web will be understood not as screenfuls of text and graphics but as a transport mechanism, the ether through which interactivity happens. It will [...] appear on your computer screen, [...] on your TV set [...] your car dashboard [...] your cell phone [...] hand-held game machines [...] maybe even your microwave oven."  Her vision began to form, as illustrated in 2004 by the O'Riley Web 2.0 conference, and since then the term has been a pivotal buzz word among the internet.  While many definitions have been provided, Web 2.0 really represents the transition from static HTML to harnessing the Internet and the Web as a platform in of itself.  

Web 2.0 provides multiple levels of application services to users across the Internet.  In essence, the web becomes an application suite for users.  Data is outsourced to wherever it is wanted, and the users have total control over what they interact with, and spread accordingly.  This requires extensive, dynamic and scalable hosting resources for these applications. This demand provides the user-base for much of the commercial Cloud computing industry today.  Web 2.0 software requires abstracted resources to be allocated and relinquished on the fly, depending on the Web's traffic and service usage at each site.  Furthermore, Web 2.0 brought Web Services standards \cite{wsci} and the Service Oriented Architecture (SOA) \cite{krafzig2004} which outline the interaction between users and cyberinfrastructure.  In summary, Web 2.0 defined the interaction standards and user base, and Grid computing defined the underlying infrastructure capabilities.  

Cloud computing \cite{Armbrust2010} is one of the most explosively expanding technologies in the computing industry today. A Cloud computing implementation typically enables users to migrate their data and computation to a remote location with some varying impact on system performance \cite{Wang2010}.  This provides a number of benefits which could not otherwise be achieved:  


\begin{itemize}

\item{\em Scalable} - Clouds are designed to deliver as much computing power as any user needs.  While in practice the underlying infrastructure is not infinite, the cloud resources are projected to ease the developer's dependence on any specific hardware.

\item{\em Quality of Service (QoS)} - Unlike standard data centers and advanced computing resources, a well-designed Cloud can project a much higher QoS than traditionally possible.  This is due to the lack of dependence on specific hardware, so any physical machine failures can be mitigated without the prerequisite user awareness.

\item{\em Specialized Environment} - Within a Cloud, the user can utilize customized tools and services to meet their needs. This can be to utilize the latest library, toolkit, or to support legacy code within new infrastructure.  

\item{\em Cost Effective} - Users finds only the hardware required for each project.  This reduces the risk for institutions potentially want build a scalable system, thus providing greater flexibility, since the user is only paying for needed infrastructure while maintaining the option to increase services as needed in the future.

\item{\em Simplified Interface} - Whether using a specific application, a set of tools or Web services, Clouds provide access to a potentially vast amount of computing resources in an easy and user-centric way. We have investigated such an interface within Grid systems through the use of the Cyberaide project \cite{las09ccgrid, las08-javascript}.

\end{itemize}




%\AJY{todo moving around, so clean up}
Many of the features noted above define what Cloud computing can be from a user perspective.  However, Cloud computing in its physical form has many different meanings and forms.  Since Clouds are defined by the services they provide and not by applications, an integrated as-a-service paradigm has been defined to illustrate the various levels within a typical Cloud, as in Figure \ref{F:layers}.

\FIGURE{htb}
 {images/cloud-layers.png}
 {1.0}
 {View of the Layers within a Cloud Infrastructure}
 {F:layers}


\begin{itemize}
\item{\em Clients} - A client interacts with a Cloud through a predefined, thin layer of abstraction.  This layer is responsible for communicating the user requests and displaying data returned in a way that is simple and intuitive for the user. Examples include a Web Browser or a thin client application.

\item{\em Software-as-a-Service (SaaS)} - A framework for providing applications or software deployed on the Internet packaged as a unique service for users to consume.  By doing so, the burden of running a local application directly on the client's machine is removed.  Instead all the application logic and data is managed centrally and to the user through a browser or thin client.  Examples include Google Docs, Facebook, or Pandora.

\item{\em Platform-as-a-Service (PaaS)} - A framework for providing a unique computing platform or software stack for applications and services to be developed on.  The goal of PaaS is to alleviate many of the burdens of developing complex, scalable software by proving a programming paradigm and tools that make service development and integration a tractable task for many.  Examples include Microsoft Azure and Google App Engine.

\item{\em Infrastructure-as-a-Service (IaaS)} - A framework for providing entire computing resources through a service.  This typically represents virtualized Operating Systems, thereby masking the underlying complexity details of the physical infrastructure.  This allows users to rent or buy computing resources on demand for their own use without needing to operate or manage physical infrastructure.  Examples include Amazon EC2, Eucalyptus, and Nimbus.

\item{\em Physical Hardware} - The underlying set of physical machines and IT equipment that host the various levels of service.  These are typically managed at a large scale using virtualization technologies which provide the QoS users expect.  This is the basis for all computing infrastructure.
\end{itemize}

When all of these layers are combined, a dynamic software stack is created to focus on large scale deployment of services to users.



%%%%%%---------------------------------------------------------------
\subsection{Infrastructure-as-a-Service}
%%%%%%---------------------------------------------------------------


Today there are a number of Clouds that offer IaaS. The Amazon Elastic Compute Cloud (EC2) \cite{www-amazon-ec2}, is probably the most popular of which and is used extensively in the IT industry. Eucalyptus \cite{nurmi2008eos} is becoming popular in both the scientific and industry communities.  It provides the same interface as EC2 and allows users to build an EC2-like cloud using their own internal resources.  Other scientific Cloud specific projects exist such as OpenNebula\cite{Fontan2008}, In-VIGO \cite{DBLP:journals/fgcs/AdabalaCCFFKMTZZZZ05}, and Cluster-on-Demand \cite{chase2003dvc}.  They provide their own interpretation of private Cloud services within a data center.  Using a Cloud deployment overlaid on a Grid computing system has been explored by the Nimbus project \cite{keahey2005vwg} with the Globus Toolkit \cite{foster1997-ijsa}. All of these clouds leverage the power of virtualization to create an enhanced data center.  The virtualization technique of choice for these Open platforms has typically been the Xen hypervisor, however more recently VMWare and the Kernel-based Virtual Machine (KVM) have become commonplace.   

%%%%%%---------------------------------------------------------------
\subsection{Workload Scheduling}
%%%%%%---------------------------------------------------------------

While virtualization provides many key advancements, this technology alone is not sufficient.  Rather, a collective scheduling and management for virtual machines is required to piece together a working Cloud.  Let us consider a typical usage for a Cloud data center that is used in part to provide computational power for the Large Hadron Collider at CERN \cite{CERN2003}, a global collaboration from more than 2000 scientists of 182 institutes in 38 nations.  Such a system would have a small number of experiments to run. Each experiment would require a very large number of jobs to complete the computation needed for the analysis.  Examples of such experiments are the ATLAS \cite{luo2005gsp} and CMS \cite{cms} projects, which (combined) require Petaflops of computing power on a daily basis.  Each job of an experiment is unique, but the application runs are often the same.  

Therefore, virtual machines are deployed to execute incoming jobs. There is a file server which provides virtual machine templates. All typical jobs are preconfigured in virtual machine templates. When a job arrives at the head node of the  cluster, a correspondent virtual machine is dynamically started on acertain compute node within the cluster to execute the job (see Figure BLAH).

While this is an abstract solution, it is important to keep in mind that these virtual machines create an overhead when compared to running on ``bare metal."  Current research estimates this the overhead for CPU bound operations at 1 to 15\% depending on the hypervisor, however more detailed studies are needed to better understand this overhead.   While the hypervisor introduces overhead, so does the actual VM image being used.  Therefore, it is clear that slimming down the images could yield an increase in overall system efficiency.  This provides the motivation for the minimal Virtual Machine image design discussed in Section \ref{sec:minvm}.

%%%%%%---------------------------------------------------------------
\subsection{Virtual Clusters}
%%%%%%---------------------------------------------------------------

TODO: write all about virtual clusters. This includes much from the Cloud Computing book that looks at virtua clusters back in 2009 and such, as well as the latest efforts with SDSC's comet cloud and various other new age efforts. Also probably good to tlak a lot aobut dynamic provisioning here? Definetely FutureGrid and Chamelian. 

Cluster computing has defined one of the core tools in distributed systems for use in parallel computation \cite{amdahl1967validity}. Cluster computing revolves around the desire to get more computing power and better reliability by utilizing many computers together across a network for 1 or many computational tasks. Clusters have manifested themselves in many different ways, ranging from Beowulf clusters \cite{becker1995beowulf} which run using commodity PCs to some of the TOP500 \cite{www-top500} supercomputing systems today.  Virtual clusters represents the growing need of users to effectively organize computational resources in an environment specific to their tasks at hand, instead of sharing a common architecture across many users. With the advent of virtualization \cite{barham2003xen}, virtual clusters have often been deployed across a set of Virtual Machines (VMs) in order to gain relative isolation and flexibility between disjoint virtual clusters. Virtual clusters, or a set of multiple cluster computing deployments on a single, larger physical cluster infrastructure, often have the following properties and attributes \cite{hwang2013distributed}:

\cite{wu2014synchronization}

\begin{itemize}
\item Resources allocation based on a VM unit
\item Clusters built of many VMs together - important mapping
\item Leverage local infrastructure management tools to provide a middleware solution for virtual clusters
	\begin{itemize}
	\item Could be a cloud IaaS such as OpenStack
	\item Or could use a queueing system such as Moab
	\end{itemize}
\item User experience based on virtual cluster management, not single VM management
\item Consolidates multiple functionality on a smaller resource platform using multiple VMs
\item Provide fault tolerance through VM migration and management
\item Dynamic scaling through the addition or deletion of VMs from the virtual cluster
\item Connection to back-end storage solution to provide virtual persistent storage
\end{itemize}

TODO: Give more info on original virtual cluster designs and implementations, such as Cluster-on-Demand, VCs in Grid computing, Nimbus, Amazon EC2, etc.


\FIGURE{!htb}
  {images/distributed_and_cloud_computing_fig318.JPG}
  {1.0}
  {Image borrowed from Cloud book - serves as temporary placeholder only}
  {F:virtualcluster}

%%%%%%---------------------------------------------------------------
\subsubsection{Performance: A first class function}
%%%%%%---------------------------------------------------------------


One of the major caveats to the usage of virtual clusters has been the performance implications. As with any systems level abstraction, virtualization technologies introduce overhead into a system and at times, can significantly degrade performance. This is especially concerning to parallel processing tasks and traditional HPC workloads, as increased performance and computational capacity are the reasons for using such systems in the first place. Workloads that depend on significant amounts of communication such as MPI based distributed memory applications and I/O heavy workloads have especially suffered from the overhead of virtualization. Traditional hypervisor overhead has has been better understood in the past as a large hurdle. The DOE Magellan project final report \cite{MegallanFinal} specifically outlined places where cloud infrastructure could, and could not fit the DOE?s scientific computing requirements. The report specifically outlines places where how virtualized infrastructure is not able to meet the needs of most HPC workloads, stating in the executive recommendations, "Virtualized cloud environments are limited by networking and I/O options available in the virtual machine." This is followed up with specific experiments regarding interconnect limitations in virtualization \cite{Ramakrishnan2012}, which rightfully show issues with existing cloud interconnect options.  However the field of virtualization has changed since the Magellan report, with many efforts undertaken to address the performance issues. 

TODO: State how Futuregrid and other efforts have found virtualization performance improving. Also, added support for InfiniBand is a game-changer for this work. Suddenly HPC is good in VMs.

\cite{hazelhurst2008scientific}
\cite{Luszczek:2011:EHC}
\cite{fox2013futuregrid}
\cite{jose2013sr}
\cite{Musleh2014cloud}

CLAIM: State recent hardware availability is driving innovation in virtual clusters

CLAIM: Mention we want to support mid-tier scientific applications. Peta-scale HPC beyond scope. Back up importance with XSEDE report showing most (90\%) of jobs on national cyberinfrastructure use less than 1024 cores. This is metric for mid-tier, and our target for deployment.  

CLAIM: consider hardware size/constraints. Physical infrastructure around petascale? Beyond single rack environments for sure. Say that while SR-IOV IB works only scale to few nodes, results are good and trends lead up to hundreds of cores possible, if not thousands. This is scope of mid-tier scientific HPC. Cite our MD paper, SDSC XSEDE paper, and NASA paper to validate claim.


\subsection{The FutureGrid Project}


FutureGrid is a national-scale Grid and Cloud test-bed facility that includes a number of computational resources across many distributed locations. The FutureGrid network is unique and can lend itself to a multitude of experiments specifically for evaluating middleware technologies and experiment management services.  This network can be dedicated to conduct experiments in isolation, using a network impairment device for introducing a variety of predetermined network conditions. Figure \ref{F:fg-map} depicts the geographically distributed resources that are outlined in Table \ref{T:fg-hardware} in more detail. All network links within FutureGrid are dedicated 10GbE links with the exception of a shared 10GbE link to TACC over the TeraGrid \cite{berman2001tkg, catlett2002philosophy} network, enabling high-speed data management and transfer between each partner site within FutureGrid.
   
\FIGURE{thb}
  {images/FG-map.pdf}
  {1.0}
  {FutureGrid Participants and Resources}
  {F:fg-map}

Although the total number of systems within FutureGrid is comparatively conservative, they provide some heterogeneity to the architecture and are connected by the high-bandwidth network links. One important feature to note is that most systems can be dynamically provisioned, e.g. these systems can be reconfigured when needed by special software that is part of FutureGrid with proper access control by users and administrators.  Therefore its believed that this hardware infrastructure can fully accommodate the needs of an experiment management system.

\begin{table*}
\caption{FutureGrid hardware}\label{T:fg-hardware}
\begin{center}
\begin{tabular}{lrrrrrll}
\hline
System type & 
\begin{sideways}Name \end{sideways}&
\begin{sideways}\# CPUs\end{sideways} & 
\begin{sideways}\# Cores \end{sideways} & 
\begin{sideways}TFLOPS	\end{sideways} & 
\begin{sideways}RAM (GB) \end{sideways} & 
\begin{sideways}Storage (TB) \end{sideways} & 
\begin{sideways} Site \end{sideways} \\
\hline
IBM iDataPlex	& India		& 256	& 1024	& 11	& 3072	& $^{\dagger}$335		& IU \\
\hline
Dell PowerEdge	& Alamo		& 192	& 1152	& 12	& 1152	& 15		        & TACC \\
\hline
IBM iDataPlex	& Hotel 	& 168	& 672	& 7	& 2016	& 120	 	        & UC \\
\hline
IBM iDataPlex	& Sierra	& 168	& 672	& 7	& 2688	& 72		& UCSD \\
\hline
Cray XT5m	& Xray		& 168	& 672	& 6	& 1344	& $^{\dagger}$335		& IU \\
\hline
ScaleMP vSMP	& Echo		& 32	& 192	& 3	& 5872	& 192		& IU \\
\hline
Dell PoweEdge	& Bravo		& 32	& 128	& 2	& 3072	& 192		& IU \\
\hline
SuperMicro	& Delta		& 32	& 192	& $^{\ddagger}$20	& 3072	& 128		& IU \\
\hline
IBM iDataPlex	& Foxtrot 	& 64	& 256	& 2	& 768	& 5 		& UF \\
\hline
\hline
Total	        & FutureGrid	& 1112	& 4960	& 70	& 23056	& 1394           & \\		
\hline
\end{tabular}

$^{\dagger}$Indicates shared file system. $^{\ddagger}$Best current estimate
\end{center}
\end{table*}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{High Performance Computing}
\label{sec:hpc}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{History of Supercomputing}



\subsection{Clusters and MMPs}


\subsection{Exascale}


