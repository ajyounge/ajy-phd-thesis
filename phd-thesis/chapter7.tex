%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Virtualization advancements to support HPC applications}
\label{chap:future-work}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

Many of the previous chapters have focused on identifying and decreasing the performance gap that exists with running HPC workloads in virtualized infrastructure, as compared to native HPC environments without virtualization. While this is a significant techincal challenge to overcome, the use of virtualization itself has the potential to offer additional benefits to HPC infrastructure. In this chapter, we look to identify research, design, and future implementation of advanced virtualization techniues to enable a new class of HPC infrastructure with added performance and features that has yet to be realized.  It may be possible for these advancements, once implemented to have an impact not only on cloud infrasturcture, but also a dedicated HPC environment for added usability and performance. 

%%%%%%---------------------------------------------------------------
\section{Memory Page Table Optimizations}
%%%%%%---------------------------------------------------------------

As we've seen both in Chapter \ref{chap:cloud2011} as well as in other supported literature \cite{MagellanFinal}, virtualization of memory structures becomes a point of contention and potential overhead. This is often due to the extensive effort a hypervisor has to perform in order to translate memory addresses from guest-virtual addresses to host-virtual addresses, and then again to machine-physical addresses.  Historically, this was handled using Shadow Page tables \cite{rosenblum2005virtual}, which eliminate the need for emulation of physical memory inside the VM by creating a page table mapping from guest virtual to machine memory. However, these page tables are not walkable by hardware such as a transition lookaside buffer (TLB), and as such guest OS page tables require updating of the shadow page table. This can be costly not only in the additional management, but also notably by the cost of VMexit and VMentry calls.

Recently, Intel and AMD have implemented Extended Page Tables (EPT) and nested paging, respectively.  With EPT, support is added to allow the TLB hardware to keep track of both guest pages and hypervisor pages concurrently, effectively removing the need for shadow page table. The downside of EPT and nested paging occurs when there is a TLB miss (the page isn't in the TLB), and as such each miss requires a walk through each VMM nested paging, effectively creating TLB miss cost of 16 table walks (instead of just 3-4) for 4k pages. While many applications find this TLB miss additional cost less much less than that of managing shadow page tables, it still can lead to a significant gap in performance between non-virtualized applications, especially as VM count or an application's memory footprint increase. 

One potential way to decrease the chance of a TLB miss (and therefore the cost of a miss) is by using a larger page size. By default, x86 hardware uses 4k pages sizes, but newer hardware can support 2M and 1G page sizes as well, effectively named \emph{transparent huge pages} or THP.  Using the KVM hypervisor with transparent hugepages enabled, we can create guest VMs backed entirely 2M hugepages \cite{Arcangeli:2010}. We can also enable transparant hugepage support within the guest as well, to have the entire guest OS (including kernel and modules) using 2M pages. 

The result of THP-enabled guest VMs can be significant. With huge pages on Intel x86 CPUs with EPT, there exists an entirely separate TLB for hugepages as well. This will naturally alleviate TLB pressure and therefore reduce TLB contention between guest and host operating systems. Because 2M pages provide larger addressable memory, the size of the page tables themselves are also decreased.  Effectively, this reduces the TLB miss cost from 4 to 3 page table walks, which when handling a VM TLB miss, then requires only 15 walks to the default 24. This in effect can have a significant improvement in VM performance, as hypothesized in \cite{Arcangeli:2010}.

To evaluate the effect of 2M transparent huge pages on guest performance, we leverage the same KVM setup in Chapter \ref{chap:cloud2014} on the Bespin hardware. Specifically, THP is switched both in the host as well as the guest OS kernels, and the same LibSVM application using GPUs is re-run. The libSVM GPU application can have significant memory requirements, as large chunks of the support vector machine datasets are stored in CPU memory and tranfered in a sudo-random order to and from GPU memory, making it an ideal application to use for evaluating THP.



 \FIGURE{htb}
  {images/libSVM-KVM-THP.pdf}
  {1.0}
  {Transparent Huge Pages with KVM}
  {F:thp}


The results of running libSVM in a THP-enabled VM, a VM with no THP, and natively without virtualization are displayed in Figure \ref{F:thp}.  Comparing first just the KVM results without THP to the native solution, we can see the impact of THP the overal application runtime, especially at larger problem sizes (6000 training sets). However, when TLB is enabled in the guest and host, we actually see the KVM VM solution \emph{outperform} the native solution. This is because guest privileged OS memory used to buffer to/from GPU memory is backed by 2M pages, instead of the normal 4k pages as in the native solution. The result is less TLB misses during application runtime compared to 4k TLB misses in a native solution, resulting in improved performance.  While this is likely a special case for THP usage with the libSVM application, the fact that a VM can even occationally outperform a native runtime is a noteworthy accomplishment. This also underscores the need for careful tuning and best-practices for hypervisors when supporting advanced scientific workloads. 

%%%%%%---------------------------------------------------------------
\section{Live Migration Mechanisms}
%%%%%%---------------------------------------------------------------

TODO: Definition of Migration, live migration, differences. (assumed known here).

Migration of VMs represents one of the fundamental advantages to virtualization, and also one of the greatest challenges to efficiency.  With VM migration, the complete VM state is copied from a source to a unallocated destination host, where disk, memory, and network connections are kept intact. For disk continuity, a distributed and/or shared filesystem is utilized, most commonly but not exclusively NFS, where both the source and destination hosts have access to the VM disk.  Network continuity is preserved so long as the destination guest is within the same LAN and generates an unsolicited ARP reply to maintain the original IP after migration.  VM vCPU states and machine states are recorded from the source and quickly sent to the destination host when the VM is paused. For live migration, the source VM is paused only after all state and memory contents are copied to the destination. The last of the dirtied memory pages are copied over, and the newly formed destination VM is then un-paused.  This pause and tranfer time represents the entirety of a VM downtime during live migration, and is often at or under 100 milliseconds across commodity Ethernet networks (given a low memory utilization).  

VM memory transfers are often the main performance consideration for overhead during live migration. This is not only due to the potentially large amount of memory to be sent across the network, but the veracity at which the memory is changed.  This is defined directly by the amount of main memory allocated (or in use) by the source VM. However, as a VM's memory is sent, the VM is still running and therefor memory pages can be dirtied, creating the need for any written page to be retransmitted. Given a small network and a memory bound processes running within a VM, this can be an infinitely long process of page dirtying. Many live migration strategies provide an iterative timeout mechanism to avoid this infinite state, but this will lead to increased downtime during migration.  

The copying of memory pages for live migration can take multiple implementaitons. Three common options are summarized below:
%ew general memory live migration concepts have been utilized, and are summarized below. 

\begin{itemize}
\item Pre-copy Migration:  All memory pages are transmitted to the destination before the VM is paused. The hypervisor will note and track all dirtied memory pages, and retransmit those pages in iterative rounds. The rounds end when either no dirtied pages exist or a max iteration count has been reached. The VM state is then transmitted and resumed on the destination. This method was the first live migration technique used in \cite{clark2005live}. 
\item Post-copy Migration: The Vm state is immediately paused and sent to the destination hypervisor, and immediately resumed. If the new destination VM generates a page fault, the VM is paused, and faulted pages are transmitted across the network on demand from the source and the VM resumed.  This methodology is proposed for use in the Xen hypervisor by Hines et al \cite{hines2009post}. 
\item Hybrid-copy Migration: Provides a compromise solution to memory paging. First, single copy of the VM memory pages, or a subset of known-necessary memory pages are sent to the destination.  Then the source VM is paused, its VM state sent to the destination, and resumed on the destination.  Known dirtied source pages, or missing pages are then copied to the destination upon a triggered page fault utilizing the same mechanism as post-copy migration. An example of hybrid migration can be found via Lu et al \cite{Lu2013}. 
\end{itemize}

While pre-copy migration is the traditional and most used live migration technique, there are opportunities about to implement other migration techniques to advance the mobility of distributed computing in high performance virtual clusters with virtualization. 



%%%%%%---------------------------------------------------------------
\subsection{RDMA-enabled VM Migration}
%%%%%%---------------------------------------------------------------

Currently, most live migration in production environments occur over TCP/IP connections, largly due to the prevelence of commodity Etherent connections within cloud infrasturcture. However even if RDMA-capable interconnects are available in such infrastructure as described with InfiniBand in Chapter \ref{chap:mdsimulations}, live migration still usually occurs over TCP/IP. For the case of InfiniBand, this is via IP over InfiniBand (IPoIB) \cite{chu2006ipoib}, which can be an inefficient use of the interconnect bandwidth and add extra latency \cite{yu2008performance}. 

The time it takes to migrate the memory contents from a source to destination VM can be significantly decrased by using an RDMA based mechanisms. Huang et al first provided a proposed pre-copy method for RDMA-based migration using the Xen hypervisor \cite{huang2007high}. Specifically, they found a 80\% decrease in migration time with RDMAwrite operations. This speedup is largely due to the removal of overhead necessary for processing TCP/IP communications, largly in CPU utilization for copying buffers, packet processing, and the included context switch overhead when competing for resources in a CPU-bound application (which are common in HPC environments). 

The live migration algorithm proposed in \cite{huang2007high} uses the standard pre-copy mechanism that sends the entire memory contents across the network as RDMA operations, then iteratively copies dirtied pages before switching the running states.  As discussed in the previous section, a post-copy migration strategy may have benefits for quick VM migration in high performance virtual clusters, or even for VM cloning as described later in Section \ref{vmcloning}. Furthermore, efforts in Chapter \ref{chap:cloud2011} have found that the Xen hypervisor is not best suited for HPC workloads.  As such, there is a need to redefine the use of RDMA for VM migration using a hybrid post-copy mechanism in a high performance hypervisor.   

\TODO{Update this item list with an algorithm definition}
\begin{itemize}
\item Transfer initial CPU state, registers
\item Start the page table pages translation process: MFN to PFN and use copy-base approach
\item Concurrently, allocate remote memory on destination VM.
\item Set up other machine state settings in destination
\item Start destination VM, pause source VM.
\item Initiate RDMAwrite of entire memory contents from source to destination
\item As page faults occur in destination VM, catch faults and perform RDMAread requesting pages. 
\end{itemize}

Currently efforts are underway to provide post-copy live migration in KVM/QEMU, using the \verb!migrate_set_capability x-postcopy-ram on! mode within KVM \cite{www-kvm-postcopy}. This method uses the Linux \emph{userfaultfd} kernel mechanisms from a kernel 4.3 or newer. At the start, all memory blocks are registered as \emph{userfaultfd} so all faults cause the running thread to pause. In kernel space, the missing page is requested from the sender, which prioritized over other pages being sent and is returned and mapped to the destination guest memory space and the thead or process is unpaused. This mechanism operates asynchronously, so that multiple outstanding page faults will not stop other executables within the VM. As such, the \verb!xpostcopy-ram! KVM extention has been identified as an ideal place to implement an RDMA based implementation. 

%Leveraging the \verb!xpost-copy-ram! effort with KVM, a post copy mechanism could be used, but instead of transfering memory pages via TCP/IP, RDMA mechanisms could be inserted. 
To provide RDMA functionalty, the InfiniBand interconnect could first be used as a proof-of-concept. This choice is due to InfiniBand's rise in popularity, increased prevelance in virtualized systems  with SR-IOV as noted in Chapter \ref{chap:mdsimulations}, and RMDA functionality. However, other interconnect opitons exist that may be better suited for enhanced functionality and performance, such as Intel's new Omnipath \cite{omnipath2015}, or an Ethernet solution such as RoCE \cite{beck2011roce}, to name a few alternatives.  While RDMA can be managed through the kernel level, it is often used in user-level APIs, such as MPI, or in the case of InfiniBand, ibVerbs. However, it may be best to select a interconnect-agnostic middlewaare for RDMA, such as Photon \cite{kissel2016photon}.



RDMA semantics can be used to either read or write contents of remote memory, in this case VM guest memory pages. However before such opeations can take place, the target side of the operation must register the remote memory buffers and send the remote key to the initiator, effectively providing the DMA addressing to be used. Beyond the increased bandwidth and decreased latency benefits provided by an advanced interconnect with InfiniBand, RDMA also allows VM memory to be sent without involving the OS. This is due to InfiniBand's xero-copy, kernel bypass mechanisms, and utilizing asynchronous operations. This keeps the CPU load down, and focused on the computation at hand rather than the I/O transfer, as it often has to when utilizing a TCP/IP stack. 
 
Selecting the proper RMDA based communication operations to use can make a notable difference in the overal performance of the migration.  
%In \cite{something}, the authors note an X\% difference between Send/Receive and Read/Write operations.   
Some RDMA operations are largely a one-sided invovement, which can have performance impact and should be carefully considered in using for post-copy live migration. The RDMAread operation requires more effort at the destiation host, while RDMA write operation puts burden on the source host.  One way to determine which method is best is by evaluating both source and sink CPU loads.  However this method likely will not be as obvious when we consider VMs will be not be running in an over-subscripbed virtualized environment, but rather a highly optimized one. 

When the destination VM page faults, it is necessary to retreive the page with as little latency as possible, as the running thread is paused. Using the method developed in \cite{www-kvm-postcopy}, the \emph{userfaultfd} will trigger an RDMAread to retreive the missing page. RDMAread requires no interaction from the source VM, as RDMAread is one sided and the memory buffers have been passed in the setup phase. To further enable quick return time for the missing page, enabling polling on the source host may further reduce latency, however verification of this will be necessary. When the RDMAread operation is finished, the running thread will resume and execution will continue.





%\TODO{Refrace to really empesize RDMA benfits provided by zero-copy, kernel bypass, and asynchronous operation( threads not blocking)}

%Considering the target of supporting HPC workloads which often have considerable resource requirements, it may be possible to assume a large destination load, as the running application is quickly resumed at the destination with a post-copy method.
% As such, RDMAread operations should be used from the destination to the source to alleviate destination host load. As it is the case in post-copy migration that the destination will trigger a page fault and needs to be handled, the RDMAread operation further makes sense to use.  This is further an important consideration when we consider performing multiple concurrent migrations, as detailed in with VM cloning later in Section \ref{vmcloning}. 


%Other porposed implementations in Xen show the advantage of RDMA pre-copy live migration 
%  - find 80\% total migration time decrase, and 77\% for applications
%  - RDMA does notransmission from the source.  
%
% require, CPU or force context switches, application performance is not impacted. This is critical for HPC
%  - Utilize zero-copy
%  - Study only looks at pre-copy methcanisms with small pages
%

%Want to provide post-copy fault based live migration for THP-backed VMs in high performant hypervisor (KVM). 






%%%%%%---------------------------------------------------------------
\subsection{Moving the Compute to the Data}
%%%%%%---------------------------------------------------------------


While pre-copy migration is dominant in the live migration techniques of nearly every mainstream hypervisor today, it is proposed that for HPC applications, post-copy migration could provide some key new advances. One particular use case would be to send a lightweight VM to directly act on a large or set of large datasets, and return a slimmed down result set. This would reduce the requirement of transmitting the data across a network entirely, and potentially speed up data access latency drastically, as on result information in the form of memory pages and VM state are transmitted. 

With post-copy migration, one could move the computation at hand close to a data source in significantly less time than full pre-copy live migration. This data source, and lightweight VM sink, could potentially be something similar to a Burst Buffer system \cite{Lofstead2014,wright2015trinity} or a classic HPC I/O node with a distribute filesystem such as Lustre or GPFS \cite{schmuck2002gpfs}. This data source could even potentially be a remote scientific instrument completely separate from the HPC infrastructure itself, especially if the network at hand is capable of RoCE \cite{beck2011roce} or iWARP\cite{rashti200710}.  A VM would initiate post-copy live migration, transmitting  only the necessary cpu state, registers, and non-paged memory rather than the full VM memory state. Once migrated, the VM could connect to a (now local) I/O or storage device, accessing data fast and perofrming the necessary calculations. The VM could potentially even forgo the copy of the majority of its memory. During this time, only the necessary memory pages required to complete the immediate calculation would generate a fault and trigger their transmission from the source.  The VM could even return the result (rather than a very large dataset) to the original source VM. 

This post-copy live migration technique for remote data computation avoids the cost of spawning a whole new job and/or process with associated running parameters, as well as the extremely high cost of a full VM live migration using the pre-copy method. However, one potential downside of post-copy live migration would be the non-deterministic runtime, as it would be unknown how much remote memory paging would be required. This could lead to more time spent with the destination VM in a paused state awaiting remote memory pages, rather than if the entire VM memory contents were transmitted completely. Careful analysis of memory usage, or a hybrid copy method based on predetermined memory sections could help overcome this issue, but require a more advanced migration architecture. 


\TODO{Should I draw this out in a process diagram?}


%TODO: Discuss work in \cite{huang2007high} and how we build upon the idea


%%%%%%---------------------------------------------------------------
\section{Fast VM Cloning}
\label{vmcloning}
%%%%%%---------------------------------------------------------------

In many distributed system environments, concurrency is achieved through the use of homogeneous compute nodes that handle the bulk of the computational load in parallel. This can take many forms, including master/slave configurations, or even a traditional HPC cluster with identical compute nodes, often used to support Single Process Multiple Data (SPMD) computational models \cite{spmd1988}. With high performance virtual clusters, there is a need to efficiently deploy and manage near identical virtual machines for distributed computation. 

In Snowflock \cite{lagar2009snowflock, lagar2011snowflock}, the notion of VM cloning is given. Specifically, Lagar et al. define the notion of VM Fork, where VMs are treated similar to a fork\(\) system call for processes.   This process is conceptually similar to VM migration, with the exception that the source VM is not destroyed after the migration. Starting with a master VM, an impromptu cluster can be created across a network using the Xen hypervisor.  Snowflock specifically uses Multicast to linearly scale out VM creation to many hosts, only coping a minimal state and then remotely coping memory pages when requested.  This fetched memory on-demand is similar in principal to the post-copy live migration technique described in the previous section.  This is further augmented with blocktap-based virtual disks with Copy-on-Write (CoW) functionally, delivering CoW slices for each child VM. 

While Snowflock provides an excellent framework for VM cloning, it is not suitable for the current implementation. First, it uses Xen, which in previous research described in Chapter \ref{chap:cloud2011} has found to have limited performance for HPC workloads \cite{Younge2011cloud}.  Second, SnowFlock is designed for Ethernet and IP based networks, which have significantly higher latency and lower bandwidth when compared to InfiniBand solutions.  Developing analogous mechanisms with a high performance hypvervisor such as KVM or Palacios \cite{lange2010palacios} to use RDMA for VM memory paging. RDMA could even be utilized in conjunction with CoW disc blocks.

Potential VM Cloning RDMA mechanism:
\begin{itemize}
\item Prepare parent VM state, including registers and info.
\item Prepare CoW disk images.
\item Create large buffers for all VM memory on child hosts. 
\item Send vmstate via RDMAwrite or IB Send to N child nodes, where N is the clone size.
\item Resume/start child VMs in tandem. 
\item Parent VM set up RDMA multicast (unreliable connection) to send memory pages to all childs efficiently
\item Child clone VM joins RDMA mlticast.
\item If child page faults, perform RDMAread operation for specific page. 
\end{itemize}

This method allows for efficient cloning of VMs based on a running parent VM. First, the VM state and images are copied to all child nodes to recieve the VM, and blank memory is allocated. Then each child VM is started, and page faults are handled via RDMA read. This will result in an initial slowdown, but as pages are received the child VMs will start to run. The rest of the memory is eventually sent via multicast to all VMs simultaniously.   As multicast is unreliable, delivery failure will just trigger a page fault and subsequent page sending via RDMA.  It allows for direct page fault handling, while allowing child VMs to start and run immediately. As RDMA multicast mechanisms are relatively questionable,  more investigation is needed to evaluate the feasibility of this situation. %A higher level API such as IBverbs or MPI may be needed for this method to effectively work.  

It is expected for post-copy VM cloning to also work most efficiencly if used in conjunction with guest VMs backed with hugepages. Transfering memory in 2M chunks will more effectively utilize network bandwdith by eliminating send/receive overhead. This also will hide the latency found in SR-IOV enabled interconnects for small messages. Furthermore, it will reduce the overhead of page fault handling mechanisms, as less overall pages will fault and be transfered.  While hugepages are expected to improve VM cloning efficency, emperical testing will still be necessary to properly evaluate their viability.   

While a VM fork mechanism leveraging post-copy live migration in KVM will quickly spool up cloned VMs, the eventual memory transfer will eventually fail to scale past the network's capacity. This could happen if hundreds or thousands of child clone VMs are being started simultaniously, as is likely in large scale deployments. As such, a higherarchical distribution may be necessary. One possible method for this would be a two-stage cloning mechanism, where child VMs are cloned one to each cabinet, and the entire memory contents copied using the pre-copy migration mechanism. From there, further cloning occurs to deploy many cloned VMs to individual nodes within the cabinet. Organization of mid-tier cloned VMs would likely be determined based on RDMA fabric configurations within cabinets, as this is a network-bound process. 

%\TODO{Describe idea for two-stage filesystem with parent disk mounted as read-only, with all write going to scratch space.  This is similar to how Cray handles node OS deployments on XC30 and XC40 systems today.}




%%%%%%---------------------------------------------------------------
\section{Virtual Cluster Scheduling}
\label{vcsched}
%%%%%%---------------------------------------------------------------

\TODO{Discuss the proximity scheduler in OpenStack, its first look. Discuss how traditional IaaS oversubscription does not fit for HPC. However, there's no reason we can't do both over-subscription for some workloads, but focus on performance for other workloads within the same infrastructure.}



%%%%%%---------------------------------------------------------------
\section{Chapter Summary}
\label{chap7sum}
%%%%%%---------------------------------------------------------------



In summary, we expect the combination of transparant huge pages, an RDMA-capable interconnect multiplexed in hardware for use by both guest and hosts, a high performant hypervisor, and post-copy migration and cloning mechanisms to enable a novel architecture for high perofrmance virtual clusters. These mechanisms, if implemented and combined with an efficient OS, such as Kitten \cite{pedretti2011kitten}, could help enable a new runtime system to support extreme scale distirbuted memory computations with in-situ analysis \cite{ahrens2010visualization, vishwanath2011toward} for large scale scientific systems. 




