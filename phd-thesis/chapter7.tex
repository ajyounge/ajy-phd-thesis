%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Virtualization advancements to support HPC applications}
\label{chap:future-work}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

Throughout this dissertation, the question of whether cloud infrastructure, using virtualization, can support mid-tier scientific computation has been investigated. These scientific problems are tightly coupled, distributed memory computations, and in the past have failed to perform well on traditional public and private cloud systems. It is our estimate that this is due to a number of reasons, including hypervisor design and implementation, lack of hardware advances, VM placement inefficiencies, and the lack of hardware availability in virtualization. In Chapter 3, we discussed the base case with off-the-shelf single node configurations.  This showed that hypervisor selection matters a great deal in performance and reliability, and that some applications can perform well on a single node. This study expressly avoided multi-node configurations due to the previously documented issues with Ethernet interconnects in virtualized environments \cite{MagellanFinal}. 

With the rise of GPUs in HPC, Chapter 4 offers a first solution in Xen to GPU passthrough as an alternative to either no GPU availability or providing GPUs access across a network by virtualizing the front end API libraries. It was found that our method of GPU passthrough incurred some overhead with Xen on older x86 hardware when transferring data across the PCI-Express bus, but provided the best option at the time.  As methods were derived for other hypervisors such as KVM and VMWare, we looked to evaluate all options for a multitude of computational problems, including both CUDA and OpenCL codes in Chapter 5. This research found that newer hardware without the QPI interconnect between the socket and the PCI-Express bus (Sandy-Bridge and up CPUs) can yield near-native performance for a range of applications.  Specifically, it was found that the KVM hypervisor was the most performant and stable hypervisor, and LXC, a container option, also performed very well (although with security limitations).  

Chapter 6 combines the lessons learned from KVM tuning, GPU passthrough, and intersects our other related efforts in inserting a high performance interconnect, in this particular case InfiniBand \cite{Musleh2014cloud}, into a virtualized cluster environment.  Specifically, a test-bed was created across 4 nodes, each with Kepler GPUs and FDR InfiniBand passed through to VMs spanning the entire CPU set. InfiniBand was specifically set up to utilize SR-IOV, which allows for the multiplexing of the ConnectX-3 VPI card to the guest instances. From here, two Molecular Dynamics simulations were run both in this tuned KVM configuration and on bare-metal.  The results indicate that overhead in virtualization is on the order of 1-2\% for these HPC applications given a few different configurations and problem sizes.   

From this work, a number of observations start to emerge. First, that virtualization may indeed be able to support HPC workloads given the advances. While the scale of the experiments of the virtual cluster is small at only a few dozen cores, the performance trends from scaling up HOOMD-blue in Figure \ref{F:HOOMD} look to be very closely correlated between the virtualized and bare metal experiments. While this application, as well as other HPC applications using similar distributed memory models, all need to be replicated at a much higher scale, we have shown there are currently no limitations in doing so, aside from the availability of the infrastructure itself.  It is our hope that with this knowledge, future test-beds can be assembled at a larger scale to further move this effort further forward.  

Another observation is that the KVM hypervisor continually proves to be the most performance-oriented hypervisor studied in this dissertation.  While this is a bit of surprise given it is a Type 2 hypervisor, which introduces additional potential for host noise and overhead, it nonetheless showed the smallest degree of variation between results, both in Chapter 3 experiments and again in Chapter 5.  KVM also offers the best performance of hypervisors overall, and with many workloads, such as seen in Figure \ref{F:SHOC_l1-2}, where KVM often performs within 0.5\% of native in SHOC benchmarks.  Furthermore, KVM has support for CPU pinning, NUMA socket binding, PCI passthrough and SR-IOV, as well as transparent huge pages and advanced migration mechanisms (described in more detail later in this Chapter).  While it is possible that other specialized hypervisors, such as Palacios \cite{lange2010palacios} (not studied), could also perform similarly, the KVM hypervisor has a large community support, industry backing, and production-level integration into the latest private infrastructure, such as OpenStack. 

A 3rd observation regarding high performance virtual clusters is that the SR-IOV InfiniBand integration described in Chapter 6 provides a drastic shift in the outlook for distributed memory applications.  While SR-IOV InfiniBand does have a 15-30\% overhead in latency compared to native implementations for small messages \cite{Musleh2014cloud}, this is still an order of magnitude better than current Ethernet options available from cloud providers such as Amazon. Furthermore, bandwidth of InfiniBand solutions in virtual clusters looks to be near-native, also surpassing current Ethernet deployments.  It is also possible for other interconnects, such as Intel's emerging Omni-Path interconnect, to demonstrate similar or better results in a virtualized ecosystem, and future experimentation should try to leverage other interconnection options if the hardware supports it.  

While these now smaller and better defined performance differences may not be satisfactory for extreme-scale distributed memory applications, we expect a large amount of users with mid-tier scientific computational problems to be accepting of these small overheads when considering the value added by working in a virtualized environment. Armed with this knowledge, we find that the outlook for high performance virtual clusters to be promising.  

However, next steps are needed to demonstrate the value of virtulaization, providing a more rich user experience while simultaneously further enhancing performance for many users.  These value-added techniques, such as efficient tuning, scheduling, VM cloning, and compute-migration may in fact help enable new classes of scientific computations, not only within HPC applications, but also with big data platform services. We specifically focus on leveraging the KVM hypervisor in conjunction with a high speed, low latency interconnect to provide new features that otherwise have yet to be made possible.  While much of this work is under construction, this nonetheless gives a glimpse at some future directions in high performance virtualization.     


%Many of the previous chapters have focused on identifying and decreasing the performance gap that exists with running HPC workloads in virtualized infrastructure, as compared to native HPC environments without virtualization. While this is a significant technical challenge to overcome, the use of virtualization itself has the potential to offer additional benefits to HPC infrastructure. 

In the rest of this chapter, we look to review the methods utilized in this dissertation regarding virtualization, then identify research, design, and future implementation of advanced virtualization techniques to enable a new class of infrastructure with added performance and features that have yet to be realized.  We focus on guest memory optimizations, live migration deployments, and integration with an RDMA-enabled interconnect for novel VM migration and cloning.  It may be possible for these advancements, once implemented, to have an impact not only on cloud infrastructure, but also on dedicated environments, which support big data applications or other distributed memory applications that focus on both usability and performance.


%%%%%%---------------------------------------------------------------
\section{PCI Passthrough}
%%%%%%---------------------------------------------------------------

As cloud computing's reach into distributed systems increases, so does the requirement of virtualization to perform at near-native speeds and to take full advantage of the underlying hardware. While this does equate to fast and efficient hypervisors, it also alludes to the effective utilization of devices beyond CPU and memory.  Such devices can vary widely and include hardware such as graphics processing units, networking adapters,  I/O hubs, web cameras, secondary and tertiary storage, to name a few. Within virtualization, oftentimes it is necessary to emulate these devices, where doing so provides a virtual hardware set that the guest VM can interact with using a specialized driver whereby the hypervisor translates requests to the underlying physical hardware.

Emulated devices are often the most common method for device interaction with VMs, and can be a critical component in virtualization. Emulated drivers create a feature set in software, where all I/O requests are intercepted by the hypervisor and emulated on the real underlying hardware. Often times, the emulated hardware provided to the guest OS within a VM is older or more generalized than the given architecture set. This is due to the fact that the effort for constructing emulated devices is large, and the emulation of older hardware often helps with overall compatibility. However, this emulation process can often be slow and  lead to significant overhead when utilizing the underlying hardware, not to mention the lack of newer features provided by more recent hardware.

Para-virtualized devices are specially tuned device software implementations of hardware where para-virtualized device drivers are installed in a guest VM that operate on a particular I/O API.  While this leads to improvements in performance over emulation methods, it requires the guest to be modified in order to communicate effectively with the hypervisor. With Xen, the entire guest OS can be para-virtualized, whereas in other solutions such as KVM using \verb|virtio|, device drivers can be para-virtualized. The performance enhancement of para-virtualization is derived from the removal of the hardware compatibility that must be in place for emulated drivers. At the cost of compatibility, it para-virtualization uses a tuned and customized API specific to the hardware at hand as an alternative.

%The performance enhancement of para-virtualization is derived from the removal of the hardware compatibility that must be in place for emulated drivers, instead potentially using a tuned and customized API specific to the hardware at hand, at the cost of compatibility.  

A recent method for guest device interaction arrives out of the use of direct I/O device passthrough, whereby the hypervisor (and controlling host OS) relinquish the entire control of a given device to the guest VM.  This allows for the guest to have direct interaction with the physical hardware, removing the need for complicated para-virtualized methods or slow emulated hardware.  As the Peripheral Component Interconnect (PCI) bus and the updated PCI-Express bus are the most common hardware interfaces for devices on modern CPU architectures, this method often viewed as PCI passthrough.  

An I/O Memory Management Unit  is responsible for managing the connection of direct memory access (DMA) capable hardware along an I/O bus to main memory. Just like a CPU MMU, the I/O MMU maps device-visible memory addresses to physical memory addresses. However, utilizing DMA-capable drivers within a guest directly without I/O MMU virtualization technology would result in the guest attempting to perform DMA operations on incorrect guest physical addresses that would not map to proper machine addresses. In order to safely and securely enable PCI passthrough in virtualized settings, such CPU architecture mechanisms are needed. 

As the popularity of virtualization increased, CPU architectures have met this demand with I/O MMU virtualization extensions. Intel and AMD have introduced VT-d and AMD-Vi (also named IOMMU) processor support for such operations.  These extensions provide the necessary mechanisms to isolate and restrict I/O devices specific to their owned partition space \cite{abramson2006vtd} through the use of I/O device assignment, DMA remapping, interrupt remapping, interrupt posting, and error reporting. DMA remapping ensures device DMAs not only find correct virtual memory addresses, but also act on only those memory addresses that are allowed, which provides the necessary VM isolation. Thus, hardware DMA remapping enables direct device assignment to VMs without device-specific knowledge in the hypervisor. 

Using these IOMMU virtualization techniques for direct I/O passthrough with various hypervisors depends greatly on the hypervisor at hand.  First, the BIOS must have IOMMU virtualization enabled and the OS kernel must initialize such hardware extensions at boot. This initialization can be done with the \verb|intel_iommu=on| Linux kernel parameter for all Intel VT-d enabled architectures.  With Xen, a specialized device driver called \verb|xen-pciback| is used to "grab" the PCIE device on boot to keep the device state uninitialized and ready to be passed through to a VM. This is described in more detail in Section 4. With KVM, a similar method is utilized, whereby either \verb|pci-stub| or \verb|vfio-pci| (the former for pre-3.9 Linux kernels, the latter for current Linux systems) is utilized to bind to the PCI device on boot before the kernel or other add-on modules attempt to initiate the device.  Either method is built into the kernel and any device drivers must be blacklisted so as to ensure proper ordering. \verb|vfio-pci|, similar to \verb|xen-pciback|, takes the PCI device ID as a parameter to determine which device to bind to ( this can be found from the output of \verb|lspci|), and holds all device initiation until the device is handed to a booting guest.  

Generally, PCI passthrough can be utilized for most PCI devices, however GPUs devices can require some added configuration and considerations.  This is largely due to the fact that GPUs are VGA devices, which represent a special case. Specifically, VGA devices to be used for PCI passthrough must not be the primary VGA displays. Second, the VGA BIOS has to be loaded by the guest VM before actual BIOS boot.  This can be done using a specific emulated BIOS, which with KVM can be either a modified SeaBIOS or a EUFI-enabled Open Virtual Machine Firmware (OVMF) configuration.  For Nvidia devices, only approved devices (often Tesla and QUADRO adapters) can be used for GPU passthrough due to proprietary VGA BIOS configurations. Of further note, some GPU devices come with attached PCI Bridges due to packaging and compatability reasons. This includes add-on GPU servers, such as the Nvidia S2050, or dual-GPU units, such as the Nvidia Tesla K80 GPU. Other devices may be packaged with such PCI Bridges for including onboard sound controllers too. However, these PCI Bridges handle PCIE connections to the devices and yet are often in separate IOMMU domains, which causes GPU passthrough to fail due to Access Controller Services (ACS) errors in the IOMMU hardware. While there are new kernel patches coming available to override the ACS mechanisms with KVM, this obviously provides a significant security vulnerability that may be problematic for deployments outside of the academic realm. 


\FIGURE{thb}
  {images/rcuda.png}
  {1.0}
  {Comparison of GPU Passthrough in Xen to rCUDA across various interconnects}
  {F:rcuda}

In Chapters 5 and 6, KVM's PCI passthrough is detailed for use with Nvidia GPUs, where we find that this method can, with NUMA placement of VMs and newer hardware on Sandy Bridge architectures, perform at near-native speeds. Without GPU passthrough, GPU usage within a virtualized domain was either not possible, or required the use of front-end API solutions. As discussed in Section 4.3 of this dissertation, such remote API methods are suboptimal due to performance considerations and the lack of full-feature support. Some of the best methods developed thus far for utilizing remote GPUs in a virtualized architecture come from the rCUDA project \cite{duato2011enabling}, which sends CUDA commands across an interconnect to a remote device. However, this approach is fundamentally limited by the interconnect itself.  Using data from the rCUDA \cite{silla2013rcuda} project, we compare GPU passthrough performance of both C2075 and K20 cards in Xen to various interconnects reliant on rCUDA.  In Figure \ref{F:rcuda}, it is illustrated that even in the best case with high speed InfiniBand, rCUDA is still limited by the interconnect fabric, which can only operate as fast as the same PCIE bus to which the GPU is attached.  In reality, even the top-end IB adapters are not capable of saturating full 16x PCIE lanes.  Furthermore,  all GPU data transfers saturate the interconnect, leaving no available bandwidth for communication operations as found with many distributed memory HPC applications.  Sockets and shared memory approaches found in API-remoting methods, such as gVirtus \cite{gvirtus}, suffer even worse performance impacts due to the necessity to buffer memory, as seen in related research \cite{walters2015gpu}.   In summary, these front-end API methods are suboptimal in comparison to GPU passthrough, as the transfer time between CPU and GPU memory often can have a drastic impact on overall application performance.

While PCI passthrough works well for providing dedicated accelerator resources such as GPUs, a sharing model of PCI passthrough does not hold. This is because PCI passthrough is a simple 1-to-1 relationship between a guest and PCI device. While it is possible to have a 1-to-many relationship (e.g. a single VM with multiple GPUs connected), there is no way to share a single device across multiple guests or the guest and the host. While this is not an issue and in fact a desirable effect with GPUs (GPUs are not designed for multi-application sharing and such a solution would largely lead to significant inefficiencies), high speed networking adapters attached on a PCIE bus do have a necessity to be shared in virtualized environments.

\FIGURE{thb}
  {images/sriov.png}
  {1.0}
  {Efficient VM networking comparison \cite{Musleh2014cloud}}
  {F:sriov}


With this requirement for sharing PCI networking adapters, a few options exist and are illustrated in Figure \ref{F:sriov}. The first option is to use software multiplexing that sorts inbound and outbound traffic and forwards packets to queues set up for each VM as necessary, as implemented with Virtual Machine Device Queues (VMDq) \cite{luo2010network}. While this solution of software queues has a workable Ethernet solution and allows VMs to share a PCI based NIC, there are performance limitations. This is due to the large amount of hypervisor involvement necessary in sorting the queues, as well as the fact that all data is buffered between the hypervisor and guest memory. The next and simplest option is simply to add more PCI adapters to a given server. While multiple PCI passthrough does work, there is a fundamental limitation to this method, as the number of cores and NUMA sockets is expanding faster than the number of PCI lanes available per socket. 
%  along with higher bandwidth NICs requiring more PCI slots. %Normal Ethernet based VM networking includes briding or tapping existing networks within the host and using driver emulation in the guest. HJowever, this method does not apply to other networks and has known perfomrance limitations. <F12>this 

With these limitations in mind, Single Root I/O Virtualization (SR-IOV) has been developed. This standardization effectively enables multiplexing of a PCI based communications adapter (typically Ethernet but also InfiniBand adapters) within the hardware.  With SR-IOV, multiple Virtual Functions (VFs) are created and configured in hardware.  Each VF has a dedicated resource pool with specific Tx and Rx queues, along with other lightweight PCI resources such as device registers, base address registers, and hardware descriptors.  Adapters also maintain a Physical Function (PF), which is a fully-implemented PCI device that acts not only as a standard controller card, but also as a control mechanism for the VFs (given firmware adjustments). These mechanisms are illustrated in Figure \ref{F:sriovpci} provided by Intel \cite{kutch2011pci}. 
 
\FIGURE{thb}
  {images/sriov-pci.png}
  {1.0}
  {SR-IOV Architecture with Virtual Functions, PCI SIG \cite{kutch2011pci}}
  {F:sriovpci}

To use SR-IOV within a virtualized setting, a VF is given to a guest VM on boot. This happens using the PCI passthrough mechanisms, except the specific VF PCI identifier is used instead of the actual adapter card. Within the guest, the VM loads a specific driver (usually supplied by the hardware vendor) that is able to probe and detect VF functionality. This driver fills in the descriptors and sets up memory allocations for DMA directly within the guest OS, effectively creating dedicated data queues within the VM.  

When a datum (could be a packet or a message, depending on the adapter type) arrives at the physical card, it is sent through a hardware switch, which places the data into a pool specific to the target VF. The data is then immediately DMAed to the guest based on the preallocated memory buffers. This is possible through the use of VT-d mechanisms which enable the translation between device addresses and the guest virtual addresses, bypassing the need for hypervisor memory translation. This entire process happens without any CPU interaction, other than occasional interrupts to provide notification of completed data work queues.   This is essentially enabling direct hardware DMA transfer to a guest without any hypervisor involvement.
 
Using modern 10GbE and InfiniBand adapters, SR-IOV can easily configurable to have up to 64 VFs, enabling a great deal of scalability within a given host.  Without any hypervisor interaction or context switching, bandwidth and latency can start to approach near-native levels, which was previously not possible with other methods. Looking at InfiniBand, research has found that Mellanox adapters with SR-IOV in KVM can achieve near-native bandwidth while incurring only a slight 15-30\% overhead on small (1 byte) messages \cite{Musleh2014cloud, jose2013sr}. This latency overhead is due to the extra hardware switching that has to happen between the VFs in the adapter itself, as well as the IOMMU involvement.   

 


%%%%%%---------------------------------------------------------------
\section{Memory Page Table Optimizations}
%%%%%%---------------------------------------------------------------

As we have seen both in Chapter \ref{chap:cloud2011}, as well as in other supported literature \cite{MagellanFinal}, virtualization of memory structures is a point of contention and potential overhead. This is often due to the extensive effort a hypervisor has to perform in order to translate memory addresses from guest-virtual addresses to host-virtual addresses, and then again to machine-physical addresses.  Whle this is a necessary function of virtualization and a hypervisor, there are various methods developed both in hardware and software to provide such address translation functionality, each with their own advantages and disadvantages. 

Modern computing systems provide a mapping of virtual memory address space to physical machine memory. This memory management technique allows processes to have independent virual address spaces, which provides security and process isolation.  Today's x86 CPUs, as well as many other CPU architectures, have a hardware memory management unit (MMU) that provides translation abilities within hardware that often have page table entries (PTE) for storing virtual to machine memory mappings, and a translation lookaside buffer (TLB) that provides an effective cache for the most commonly used virtual addresses.  Specifically, x86 page tables are walked iteratively in hardware with their layout specified by the x86 hardware specification where the CR3 register holds the page table base and a 4-level radix tree structure represents the page table hierarchy for 4KB pages.  

With virtual machines, a 2-level address translation is needed where guest virtual memory is translated to guest physical memory and then again to physical machine memory. This direct two-level memory mapping is classically handled by the hypervisor, as the guest cannot access machine memory directly. The hypervisor functions in software without hardware support, so it can be expensive for it to update and maintain guest memory translation. With x86 virtualization, memory virtualization is handled using shadow page tables \cite{rosenblum2005virtual}. Shadow page tables eliminate the need for emulation of physical memory inside the VM by creating a page table mapping from guest virtual to machine memory. However, these page tables are not walkable by hardware like a TLB and, as such, guest OS page tables require updating of the shadow page table by the hypervisor. This can be costly not only in the additional management, but also by the cost of VMexit and VMentry calls, which are known to add thousands of CPU cycles of overhead for each call.  If there is a memory bound application, continual shadow page table management by the hyperivsor can add a notable overhead, impacting overall application performance.

Recently, Intel and AMD have implemented Extended Page Tables (EPT) and nested paging (NPT), respectively, to cope with the issues of shadow page tables.  With nested paging, a guest page table converts guest virtual addresses to get physical addresses, and another second level table converts guest physical addresses to machine addresses.  Each address translation in guest mode requires a 2D page walk where the guest page table is traversed and each guest physical address requires a second level page table walk to obtain the machine address. This support means the TLB hardware is able to keep track of both guest pages and hypervisor pages concurrently, effectively removing the need for shadow page tables and resulting hypervisor intervention entirely.  Nested paging provides a simple design with no necessary hypervisor traps leading to less overal overhead and swapping, less TLB flushes, and a reduced memory footprint.

The downside of nested paging for virtual machines occurs when there is a TLB miss. A TLB miss is when a requested page is not in the TLB, and in such cases the cost of a TLB miss is substantially higher in a guest VM. Natively, a TLB miss requires a walk of 4 address entries for 4KB pages. However, in a guest, each of those 4 address entires require another second level walk to find the system physical address for each guest page entry, plus a final nested page walk to translate the final guest physical address to a usable machine address.  

The TLB miss can be illustrated in the efforts in Figure \ref{F:npt} from Bhargava et. al \cite{bhargava2008npt}, whereby we can see the steps for handling a TLB miss on an AMD CPU in both native (left) and virtualized (right) modes. With 2D nested or extended page tabling given in Figure \ref{F:npt}, each \verb|gLn| entry cannot be directly read using a guest physical address, and, as such, the nested page table walk is necessary to translate the guest physical address before the entry can be read. This has to happen recursively for each level, and in this example with 4KB pages, that includes 4 levels. Lastly, a final nested page walk is required to translate the guest physical address of the data to a physical machine address.  

\FIGURE{thb}
  {images/npt.png}
  {1.0}
  {Native TLB miss walk compared with 2D virtualized TLB miss walk from \cite{bhargava2008npt}. On the left illustrates a native page table walk. On the right illustrates the lengthy 2D nested page table walk for a VM.}
  {F:npt}


The cost of a 2D page table can be evaluated in the number of references, and is easily calculated. If a guest page walk has \verb|n| levels and a nested page walk has \verb|m| levels, the virtualized 2D walk has a cost calculated by:
 
\begin{verbatim}

nm + n + m
\end{verbatim}

%\verb|nm + n + m|

For 4KB pages, this requires 24 references in a virtual TLB miss (4 page waks and 4 nested page walks), compared to a cost of just 4 references for a single TLB walk natively, as indicated in Figure \ref{F:npt}. While many applications illustrate this TLB miss cost is much less than that of managing shadow page tables, it can still lead to a significant gap in performance between non-virtualized applications, especially as VM count or an application's memory footprint increases. 

One potential way to decrease the chance of a TLB miss (and therefore the cost of a miss) is to use a larger page size. By default, x86 hardware uses 4KB pages sizes, but newer hardware can support 2M and 1G page sizes as well, effectively named \emph{transparent huge pages} or THP.  Using the KVM hypervisor with transparent huge pages enabled, we can create guest VMs backed entirely by 2M huge pages \cite{Arcangeli:2010}. We can also enable transparent huge page support within the guest, as well, to have the entire guest OS (including kernel and modules) backed by 2M pages. 

The result of THP-enabled guest VMs can be significant. With huge pages on Intel x86 CPUs with EPT, there exists an entirely separate TLB for huge pages. This will naturally alleviate TLB pressure and therefore reduce TLB contention between guest and host operating systems (whereby the host is still utilizing 4KB paging for kernel space). Most importantly, the TLB reach is increased, because 2MB pages cover a larger addressable memory space.  The size of the page tables themselves also decreases with the use of huge pages.  

If huge pages are used in the host, there are then 3 levels of a page table walk, and if huge pages are used in the guest, there is also only a nested page walk of 3 levels. Using the formula, if THP is enabled in the host, there is a TLB miss cost of 19 references.  If huge page support is enabled in both the guest and the host, this drops to only 15 references.  While this is still more than the native miss cost of 4 or 3 references (for 4KB and 2MB pages, respectively), huge pages provide a 37.5\% reduction in the TLB miss cost compared to 4KB pages, still significantly better than the VMexit/entry costs associated with shadow page tables.  While it would be possible to only enable huge pages within the guest, this would represent a suboptimal configuration. Here, 2MB pages will be splintered into 4KB pages, thus negating most performance benefits \cite{pham2015splintering}.  Most importantly, the overall reduction of the number of TLB misses due to the increased TLB reach with huge pages can have a major impact on application performance.  

%REWRITTENWith transparent huge pages, the TLB reach is increased, and because 2M pages provide larger addressable memory, the size of the page tables themselves are also decreased.  Effectively, this reduces the TLB miss cost from 4 to 3 page table walks, which, when handling a VM TLB miss, then requires only 15 walks to the default 24. This, in effect, can have a significant improvement in VM performance \cite{Arcangeli:2010}.

To evaluate the effect of 2M transparent huge pages on guest performance, we leverage the same KVM setup in Chapter \ref{chap:cloud2014} on the Bespin hardware. Specifically, THP is enabled both in the host as well as the guest OS kernels, and the same LibSVM application using GPUs is re-run. The libSVM GPU application can have significant memory requirements, as large chunks of the SVM datasets are stored in CPU memory and transferred in a sudo-random order to and from GPU memory, making it an ideal application to use for evaluating THP.



 \FIGURE{htb}
  {images/libSVM-KVM-THP.pdf}
  {1.0}
  {Transparent Huge Pages with KVM}
  {F:thp}


The results of running libSVM in a THP-enabled VM, a VM with no THP, and natively without virtualization are all outlined in Figure \ref{F:thp}.  Comparing first just the KVM results without THP to the native solution, we can see the impact of THP on the overall application runtime, especially at larger problem sizes (6000 training sets). However, when TLB is enabled in the guest and host, we actually see the KVM VM solution \emph{outperform} the native solution. This is because guest privileged OS memory used to buffer to/from GPU memory is backed by 2MB pages in kernel space, instead of the normal 4k pages, as in the native solution (which has to use 4KB pages). This means data transfers can take advantage of a larger TLB reach, resulting in improved performance.  While this is likely a special case for THP usage with the libSVM application, the fact that a VM can even occasionally outperform a native runtime is a noteworthy accomplishment. This also underscores the need for careful tuning and best-practices for hypervisors when supporting advanced scientific tasks where huge page support in both the guest and host environments as a key aspect, especially as the use of big data scientific applications increases.  

%%%%%%---------------------------------------------------------------
\section{Time in Virtual Machines}
%%%%%%---------------------------------------------------------------

The measurement of time has been a longstanding consideration within Distributed Systems for many years.  Time synchronization has often been at the forefront of that effort, with Lamport clocks \cite{lamport1978time} often used to illustrate a classic example of how critically important time is within such large scale systems.  However, even simply keeping track of time and receiving fine graned resolution can often be difficult.  

Within modern CPUs, time is measured a few different ways, each with their own advantages and disadvantages.  Classically, time was measured in clock cycles for small operations typically within a OS kernel using a real-time clock (RTC) where an operation's time is measured by simply reading the number of clock cycles before and after an operation. This worked well for timing tasks where actual wall clock time was not needed, but simply a metric to compare operation length between implementations. However, with CPU improvements came dynamic voltage and frequency scaling as well as CPU sleep states, which could and often would confound clock cycle measurements.  If a CPU entered a sleep state, it was not guaranteed that the cycle counter would continue to be updated. 

Recent x86 CPUs now use a Time Stamp Counter (TSC), first introduced with the Pentium processor. The TSC is incremented at a fixed clock rate, independent of actual frequency and is not halted. The TSC essentially provides time sensitive information within hardware, rather than software, leading to fine granularity. Within Linux, if \verb|constant_tsc| and \verb|nonstop_tsc| are specified, the TSC can generally be relied upon for accurate measurements independent of frequency or sleep states. In such a case, the TSC can be essentially utilized as a clock source. Reading of the TSC is done through the \verb|rdtsc| instruction, itself generally taking about 24 clock cycles to complete, meaning it is a relatively fine-grained operation for time measurement.

There is also a High Precision Event Timer (HPET) that is used in some x86 systems for the past decade. This provides nanosecond granularity and utilizes multiple hardware sources to provide an accurate time measurement. As such, HPET was a desirable tool for time measurement before an invaraent TSC was implemented. Furthermore, HPET is synchronous across multiple cores, whereas a TSC was not until very recently.  Disappointingly, recent many-core architectures such as the Intel Knights Landing standalone CPUs still do not provide a core synchronized TSC, and as such the HPET may be more desirable to use for multi-core measurements. However, the overhead for HPET can be much larger than TSC, and implementations vary significantly on the performance, leading to some problems for very fine-grained measurements. 

While clock timing and measurements on bare-metal OS and services is now relatively straightforward, virtualized time is not always as straightforward.  This is because as the hypervisor virtualizes a given CPU, it also has to provide some virtualized notion of such time counters and clocks to the virtual machines as well.  Until recently, many hypervisors virtualized such timing systems in many different ways. 

For instance, VMWare provides a virtual TSC by default. This virtual TSC was particularly useful when multi-core TSC were not reliable and there was a need for timing within VMWare VMs. However, there are a number of problems with VMWare's virtual TSC. The VMWare hypervisor has to perform work to virtualize the TSC for each read.  For fine-grained measurements that are in close proximity, this can cause a backlog in the hypervisor, and the TSC can fall behind real time. In effect, for sufficiently small measurements VMWare can report better than native performance when benchmarking.  However, this is actually an atifact of VMWare's virtual TSC, not something that is consistent in real time. This problem has lead to the removal for VMWare results throughout this manuscript as VMWare timing results cannot be relied upon. Very recently, the use of x86 invariant TSC by other hypervisors has lead VMWare to try to address the issues with its virtual TSC by providing a mechanism within ESXi to forcibly disable the virtual TSC.  However, the results reported within this dissertation have not looked to reevaluate to confirm this method in fact is now reliable. 

While VMWare's virtual TSC is problematic, other hypervisors such as KVM have to also wrangle the same situation that lead to VMWare's design of the virtual TSC. However KVM takes a much more simple approach of providing the TSC directly to the guest VM. Intel's VMX instruction set provides conditional trapping of RDTSC, RDMSR, WRMSR and RDTSCP instructions, which is enough for full virtualization of TSC in any manner. VMX also allows for a offset field, to allow for the guest to synchronize TSC of multiple vCPUs. These mechanisms allow KVM to create a simple mapping between guest and host TSC, even if the TSC can be written by either.  However as noted earlier with real-time clocks and TSCs on bare metal, the TSCs can get out of sync when power-saving modes are applied, such as with frequency states. As such, KVM best practices include the disabling of all power-saving features to ensure the TSC remains constant and reliable.  With \verb|constant_tsc| utilized, this situation is no longer a problem in newer CPU architectures. For all experiments in this dissertation, power saving modes were disabled in the host and guest to ensure no invalid results.  



%%%%%%---------------------------------------------------------------
\section{Live Migration Mechanisms}
%%%%%%---------------------------------------------------------------

%TODO: Definition of Migration, live migration, differences. (assumed known here).

Migration of VMs represents one of the fundamental advantages to virtualization, and also one of the greatest challenges to efficiency.  With VM migration, the complete VM state is copied from a source to an unallocated destination host, where disk, memory, and network connections are kept intact. For disk continuity, a distributed and/or shared filesystem is utilized, most commonly but not exclusively NFS, where both the source and destination hosts have access to the VM disk.  Network continuity is preserved as long as the destination guest is within the same LAN and generates an unsolicited ARP reply to maintain the original IP after migration.  VM vCPU states and machine states are recorded from the source and are quickly sent to the destination host when the VM is paused. For live migration, the source VM is paused only after all state and memory contents are copied to the destination. The last of the dirtied memory pages are copied over, and the newly formed destination VM is then un-paused.  This pause and transfer time represents the entirety of a VM downtime during live migration, and is often at or under 100 milliseconds across commodity Ethernet networks (given a VM with low memory utilization).  

The memory transfer stages are often the main performance consideration for overhead during live migration. This is not only due to the potentially large amount of memory to be sent across the network, but also the veracity at which the memory is changed.  This is defined directly by the amount of main memory allocated (or in use) by the source VM. However, as a VM's memory is sent, the VM is still running and therefor memory pages can be dirtied, creating the need for any written page to be retransmitted. Given a small network and memory bound processes running within a VM, this can be an infinitely long process of page dirtying. Many live migration strategies provide an iterative timeout mechanism to avoid this infinite state, but this will lead to increased downtime during migration.  

The copying of memory pages for live migration can take multiple implementations. Three common options are summarized below:
%ew general memory live migration concepts have been utilized, and are summarized below. 

\begin{itemize}
\item \textbf{Pre-copy Migration - }  All memory pages are transmitted to the destination before the VM is paused. The hypervisor will note and track all dirtied memory pages, and retransmit those pages in iterative rounds. The rounds end when either no dirtied pages exist or a max iteration count has been reached. The VM state is then transmitted and resumed on the destination. This method was the first live migration technique used in Clark et al \cite{clark2005live} and is by far the most common. 
\item \textbf{Post-copy Migration - } The VM state is paused and sent to the destination hypervisor, and immediately resumed. If the new destination VM generates a page fault, the VM is paused, and faulted pages are transmitted across the network on demand from the source and the VM resumed.  This methodology is proposed for use in the Xen hypervisor by Hines et al \cite{hines2009post}. 
\item \textbf{Hybrid-copy Migration - } Provides a compromise solution to memory paging. First, a single copy of the VM memory pages, or a subset of known-necessary memory pages, are sent to the destination.  Then the source VM is paused, its VM state sent to the destination, and resumed on the destination.  Known dirtied source pages, or missing pages, are then copied to the destination upon a triggered page fault utilizing the same mechanism as post-copy migration. An example of hybrid migration can be found via Lu et al \cite{Lu2013}. 
\end{itemize}

While pre-copy migration is the traditional and most used live migration technique, there will soon be opportunities to implement other migration techniques to advance the mobility of distributed computing in high performance virtual clusters with virtualization. 



%%%%%%---------------------------------------------------------------
\subsection{RDMA-enabled VM Migration}
%%%%%%---------------------------------------------------------------

Currently, most live migration in production environments occurs over TCP/IP connections due to the prevalence of commodity Ethernet connections within cloud infrastructure. However, even if RDMA-capable interconnects are available in such infrastructure as described with InfiniBand in Chapter \ref{chap:mdsimulations}, live migration still usually occurs over TCP/IP. For the case of InfiniBand, this is via IP over InfiniBand (IPoIB) \cite{chu2006ipoib}, which can be an inefficient use of the interconnect bandwidth and add extra latency \cite{yu2008performance}. 

The time it takes to migrate the memory contents from a source to destination VM can be significantly decreased by using an RDMA based mechanisms. Huang et al first provided a proposed pre-copy method for RDMA-based migration using the Xen hypervisor \cite{huang2007high}. Specifically, they found an 80\% decrease in migration time with RDMAwrite operations. This speedup is largely due to the removal of overhead necessary for processing TCP/IP communications, largely in CPU utilization for copying buffers, packet processing, and the included context switch overhead when competing for resources in a CPU-bound application (which are common in HPC environments). 

The live migration algorithm proposed in \cite{huang2007high} uses the standard pre-copy mechanism that sends the entire memory contents across the network as RDMA operations, then iteratively copies dirtied pages before switching the running states.  As discussed in the previous section, a post-copy migration strategy may have benefits for quick VM migration in high performance virtual clusters, or even for VM cloning as described later in Section \ref{vmcloning}. Furthermore, efforts in Chapter \ref{chap:cloud2011} have found that the Xen hypervisor is not best suited for HPC workloads.  As such, there is a need to redefine the use of RDMA for VM migration using a hybrid post-copy mechanism in a high performance hypervisor. This post-copy migration mechanism is provided defined:
%\TODO{Update this item list with an algorithm definition}

\begin{itemize}
\item Transfer initial CPU state, registers
\item Start the page table pages translation process: MFN to PFN and use copy-base approach
\item Concurrently, allocate remote memory on destination VM.
\item Set up other machine state settings in destination
\item Start destination VM, pause source VM.
\item Initiate RDMAwrite of entire memory contents from source to destination.
\item As page faults occur in destination VM, catch faults and perform RDMAread requesting pages. 
\end{itemize}

Currently efforts are underway to provide post-copy live migration in KVM/QEMU, using the \verb!migrate_set_capability x-postcopy-ram on! mode within KVM \cite{www-kvm-postcopy}. This method uses the Linux \emph{userfaultfd} kernel mechanisms from a kernel 4.3 or newer. At the start, all memory blocks are registered as \emph{userfaultfd}, so all faults cause the running thread to pause. In kernel space, the missing page is requested from the sender, which is prioritized over other pages being sent and is returned and mapped to the destination guest memory space and the thread or process is un-paused. This mechanism operates asynchronously, so that multiple outstanding page faults will not stop other executables within the VM. The \verb!xpostcopy-ram! extension has been identified as an opportune place to implement an RDMA based implementation within KVM. 

%Leveraging the \verb!xpost-copy-ram! effort with KVM, a post copy mechanism could be used, but instead of transferring memory pages via TCP/IP, RDMA mechanisms could be inserted. 
To provide RDMA functionality, the InfiniBand interconnect could first be used as a proof-of-concept. This choice is due to InfiniBand's rise in popularity, increased prevalence in virtualized systems with SR-IOV, as noted in Chapter \ref{chap:mdsimulations}, and RMDA functionality. However, other interconnect options exist that may be better suited for enhanced functionality and performance, such as Intel's new Omnipath \cite{omnipath2015}, or an Ethernet solution such as RoCE \cite{beck2011roce}, to name a few.  While RDMA can be managed through the kernel level, it is best used in user-level APIs, such as MPI, or in the case of InfiniBand, ibVerbs. However, it may be ideal to select a interconnect-agnostic middleware for RDMA implementations to add future support for other interconnects, such as Photon \cite{kissel2016photon}.



RDMA semantics can be used to either read or write contents of remote memory, in this case VM guest memory pages. However before such operations can take place, the target side of the operation must register the remote memory buffers and send the remote key to the initiator, effectively providing the DMA addressing to be used. Beyond the increased bandwidth and decreased latency benefits provided by an advanced interconnect with InfiniBand, RDMA also allows VM memory to be sent without involving the OS. This is due to InfiniBand's zero-copy, kernel bypass mechanisms, and asynchronous operations. This keeps the CPU load down, allowing for the CPU to spend time on the computation at hand rather than the I/O transfer, as it often has to when utilizing a TCP/IP stack. 
 
Selecting the proper RMDA based communication operations to use can make a notable difference in the overall performance of the migration.  
%In \cite{something}, the authors note an X\% difference between Send/Receive and Read/Write operations.   
Some RDMA operations are largely a one-sided involvement, which can have performance impact and should be carefully considered for use in post-copy live migration. The RDMAread operation requires more effort at the destination host, while RDMAwrite operation puts burden on the source host.  One way to determine which method is best is by evaluating both source and sink CPU loads.  However, this method likely will not be as obvious when we consider VMs will be not be running in an over-subscribed virtualized environment, but rather a highly optimized one. 

When the destination VM page faults, it is necessary to retrieve the page with as little latency as possible, as the running thread is paused. This is one of the advantages of using RDMA, but implementation details can also effect performance. Using the method developed in \cite{www-kvm-postcopy}, the \emph{userfaultfd} will trigger an RDMAread to retrieve the missing page. RDMAread requires no interaction from the source VM, as RDMAread is one sided and the memory buffers have been passed in the setup phase. To further enable quick return time for the missing page, enabling polling on the source host may further reduce latency, however verification of this will be necessary. When the RDMAread operation is finished, the running thread will resume and execution will continue.





%\TODO{Refrace to really empesize RDMA benfits provided by zero-copy, kernel bypass, and asynchronous operation( threads not blocking)}

%Considering the target of supporting HPC workloads which often have considerable resource requirements, it may be possible to assume a large destination load, as the running application is quickly resumed at the destination with a post-copy method.
% As such, RDMAread operations should be used from the destination to the source to alleviate destination host load. As it is the case in post-copy migration that the destination will trigger a page fault and needs to be handled, the RDMAread operation further makes sense to use.  This is further an important consideration when we consider performing multiple concurrent migrations, as detailed in with VM cloning later in Section \ref{vmcloning}. 


%Other porposed implementations in Xen show the advantage of RDMA pre-copy live migration 
%  - find 80\% total migration time decrase, and 77\% for applications
%  - RDMA does notransmission from the source.  
%
% require, CPU or force context switches, application performance is not impacted. This is critical for HPC
%  - Utilize zero-copy
%  - Study only looks at pre-copy methcanisms with small pages
%

%Want to provide post-copy fault based live migration for THP-backed VMs in high performant hypervisor (KVM). 






%%%%%%---------------------------------------------------------------
\subsection{Moving the Compute to the Data}
%%%%%%---------------------------------------------------------------


While pre-copy migration is dominant in the live migration techniques of nearly every mainstream hypervisor today, the proposed post-copy migration could provide some key new advances for HPC and big data applications. One particular use case would be to send a lightweight VM to act directly on a large or set of large datasets and return a slimmed down result set. This would reduce the requirement of transmitting the data across a network entirely, and could potentially speed up data access latency drastically, as the returning information is transmitted in the form of memory pages and VM state. Essentially, the proposal is to send the compute to the data, instead of visa versa.  

With post-copy migration, one could move the computation at hand close to a data source in significantly less time than full pre-copy live migration. This data source, and lightweight VM sink, could potentially be something similar to a Burst Buffer system \cite{Lofstead2014,wright2015trinity} or a classic HPC I/O node with a distribute filesystem such as Lustre or GPFS \cite{schmuck2002gpfs}. This data source could even potentially be a remote scientific instrument completely separate from the HPC infrastructure itself, especially if the network at hand is capable of RoCE \cite{beck2011roce} or iWARP\cite{rashti200710}.  A VM would initiate post-copy live migration, transmitting  only the necessary CPU state, registers, and non-paged memory, rather than the full VM memory state. Once migrated, the VM could connect to a (now local) I/O or storage device, accessing data fast and performing the necessary calculations. The VM could potentially even forgo the copy of the majority of its memory. During this time, only the necessary memory pages required to complete the immediate calculation would generate a fault and trigger their transmission from the source.  The VM could even return the result (rather than a very large dataset) to the original source VM. 

This post-copy live migration technique for remote data computation avoids the cost of spawning a whole new job and/or process with associated running parameters, as well as the extremely high cost of a full VM live migration using the pre-copy method. However, one potential downside of post-copy live migration would be the non-deterministic runtime, as it would be unknown how much remote memory paging would be required. This could lead to more time spent with the destination VM in a paused state awaiting remote memory pages, rather than if the entire VM memory contents were transmitted completely. Careful analysis of memory usage, or a hybrid copy method based on predetermined memory sections, could help overcome this issue, but may require a more advanced migration architecture. 


%\TODO{Should I draw this out in a process diagram?}


%TODO: Discuss work in \cite{huang2007high} and how we build upon the idea


%%%%%%---------------------------------------------------------------
\section{Fast VM Cloning}
\label{vmcloning}
%%%%%%---------------------------------------------------------------

In many distributed system environments, concurrency is achieved through the use of homogeneous compute nodes that handle the bulk of the computational load in parallel. This can take many forms, including master/slave configurations, or even a traditional HPC cluster with identical compute nodes, as are often used to support Single Process Multiple Data (SPMD) computational models \cite{spmd1988}. With high performance virtual clusters, there is a need to efficiently deploy and manage near identical virtual machines for distributed computation. 

In Snowflock \cite{lagar2009snowflock, lagar2011snowflock}, the notion of VM cloning is given. Specifically, Lagar et al. define the notion of VM Fork, where VMs are treated similarly to a fork\(\) system call for processes.   This process is conceptually similar to VM migration, with the exception being that the source VM is not destroyed after the migration. Starting with a master VM, an impromptu cluster can be created across a network using the Xen hypervisor.  Snowflock specifically uses Multicast to linearly scale out VM creation to many hosts, only coping a minimal state and then remotely coping memory pages when requested.  This fetched memory on-demand is similar in principal to the post-copy live migration technique described in the previous section.  This is further augmented with blocktap-based virtual disks with Copy-on-Write (CoW) functionally, delivering CoW slices for each child VM. 

While Snowflock provides an excellent framework for VM cloning, it is not suitable for the current implementation. First, it uses Xen, which in previous research described in Chapter \ref{chap:cloud2011} has been found to have limited performance for HPC workloads \cite{Younge2011cloud}.  Second, SnowFlock is designed for Ethernet and IP based networks, which have significantly higher latency and lower bandwidth when compared to InfiniBand solutions.  Developing analogous mechanisms, like what is listed below, with a high performance hypvervisor such as KVM or Palacios \cite{lange2010palacios} to use RDMA for VM memory paging could have an effect on the way in which virtual cluster environments are deployed. %RDMA could even be utilized in conjunction with CoW disc blocks.

\begin{itemize}
\item Prepare parent VM state, including registers and info.
\item Prepare CoW disk images.
\item Create large buffer for all VM memory on child hosts. 
\item Send vmstate via RDMAwrite or IB Send to N child nodes, where N is the clone size.
\item Resume/start child VMs in tandem. 
\item Parent VM set up RDMA multicast (unreliable connection) and initiiate sending of memory pages to all child VMs 
\item Child clone VM joins RDMA multicast.
\item If child page faults, perform RDMAread operation for specific page. 
\end{itemize}

This method allows for efficient cloning of VMs based on a running parent VM. First, the VM state and images are copied to all child nodes to receive the VM, and blank memory is allocated. Then, each child VM is started, and page faults are handled via RDMA read. This will result in an initial slowdown, but as pages are received the child VMs will start to run. The rest of the memory is eventually sent via multicast to all VMs simultaneously.   As multicast is unreliable, a delivery failure will just trigger a page fault and subsequent page retransmission via RDMA.  It allows for direct page fault handling, while still allowing child VMs to start and run immediately. As RDMA multicast mechanisms are relatively questionable,  more investigation is needed to evaluate the feasibility of this situation. %A higher level API such as IBverbs or MPI may be needed for this method to effectively work.  

It is expected that post-copy VM cloning will work most efficiently if used in conjunction with guest VMs backed with huge pages. Transferring memory in 2M chunks will more effectively utilize network bandwidth by eliminating send/receive overhead. This also will hide the latency found in SR-IOV enabled interconnects for small messages. Furthermore, it will reduce the overhead of page fault handling mechanisms, as less overall pages will fault and be transferred.  While huge pages are expected to improve VM cloning efficiency, empirical testing will still be necessary to properly evaluate their viability.   

While a VM fork mechanism leveraging post-copy live migration in KVM will quickly spool up cloned VMs, the eventual memory transfer will eventually fail to scale past the network's capacity. This could happen if hundreds or thousands of child clone VMs are started simultaneously, as is likely in large scale deployments. As such, a hierarchical distribution may be necessary. One possible method for this would be a two-stage cloning mechanism, where child VMs are cloned one to each cabinet, and the entire memory contents copied using the pre-copy migration mechanism. From there, further cloning occurs to deploy many cloned VMs to individual nodes within the cabinet. Organization of mid-tier cloned VMs would likely be determined based on RDMA fabric configurations within cabinets, as this is a network-bound process. 

%\TODO{Describe idea for two-stage filesystem with parent disk mounted as read-only, with all write going to scratch space.  This is similar to how Cray handles node OS deployments on XC30 and XC40 systems today.}




%%%%%%---------------------------------------------------------------
\section{Virtual Cluster Scheduling}
\label{vcsched}
%%%%%%---------------------------------------------------------------

%\TODO{Discuss the proximity scheduler in OpenStack, its first look. Discuss how traditional IaaS oversubscription does not fit for HPC. However, there's no reason we can't do both over-subscription for some workloads, but focus on performance for other workloads within the same infrastructure.}

Historically, cloud infrastructure has taken a simplistic approach when it comes to VM and workload scheduling. Often, round-robin or greedy \cite{Younge2011eagc} scheduling algorithms are naively applied. With round robin scheduling, a simple list of host machines are used and iterated over as VM requests are made. This essentially scatters the VMs without regard to their locality, and over-subscription becomes commonplace regardless of the number of requested VMs or their interconnection. A greedy algorithm can help keep spacial locality, but focuses specifically on over-subscription as well to help consolidate VM allocations. While this over-subscription aspect is advantageous for public cloud providers such as Amazon EC2, it becomes counterproductive for high performance virtual clusters.

With high performance virtual clusters, VM instances that can gain near-native performance are needed. Furthermore, these VMs will be running tightly coupled applications, and, as such, must be allocated in a way in which communication latency is minimized and bandwidth is maximized for all VMs. This will help insure the entire virtual clusters can perform optimally.  However, given off-the-shelf private cloud providers or, worse still, public cloud infrastructure, these requirements are at best opaque to the user, and at worst extremely suboptimal.

One way in which high performance virtual clusters can operate efficiently is by defining specific instance types, or \emph{flavors} within OpenStack, that define what resources a VM has allocated.  Given previous research on the NUMA effects of VMs \cite{openstack-numa}, these specialized flavors should be defined to fit within a NUMA socket.  Furthermore, CPU pinning should be used to specifically keep the VM within the NUMA socket itself.  Using Libvirt, a common API utilized in many cloud infrastructure deployments (including OpenStack), we can specify CPU pinning directly in the XML configuration.

\begin{verbatim}
<cputune>
 <vcpupin vcpu="0" cpuset="0"/>
 <vcpupin vcpu="1" cpuset="1"/>
 <vcpupin vcpu="2" cpuset="4"/>
 <vcpupin vcpu="3" cpuset="5"/>
 <vcpupin vcpu="4" cpuset="6"/>
 <emulatorpin cpuset="2"/>
</cputune>
\end{verbatim}

\FIGURE{thb}
  {images/extra_specs.png}
  {1.0}
  {Adding extra specs to a VM flavor in OpenStack}
  {F:extraspecs}

With OpenStack, this NUMA configuration can be executed through the KVM Nova plugin and a scheduling filter, which defines how to place VMs effectively. Furthermore, the instance flavor can also specify the addition of \verb!instance_type_extra_specs! within Nova, whereby specialized hardware such as GPUs and InfiniBand interconnects (as described in Chapters 5 and 6) can be passed through to the VMs directly. Once defined, the same specialized high performance flavors in OpenStack simply have to add the labels (such as 'gpu' or 'infiniband') to the flavor to gain the requested hardware, as illustrated in Figure \ref{F:extraspecs} with OpenStack Horizon's UI interface.  The implementation put forth in the OpenStack Havana build has since been upgraded by Intel with SR-IOV support and additional scheduling filter additions, and is available in the latest OpenStack releases \cite{jiang2015}. 

\begin{verbatim}
pci_passthrough_devices=[{"label":"gpu","address":"0000:08:00.0"},
			 {"label":"infiniband","address":"0000:21:00.0"}]
instance_type_extra_specs={'pci_passthrough:labels':'["gpu"]'}
instance_type_extra_specs={'pci_passthrough:labels':'["infiniband"]'}
\end{verbatim}
 

To provide a space for high performance virtual clusters, we need a scheduling mechanism within a cloud infrastructure to support the effective and proper placement beyond controlling for NUMA characteristics. While there are many effective scheduling algorithms for workload placement within an environment, a Proximity Scheduler \cite{www-proximity-scheduler}, as defined in OpenStack, may work well. Specifically, one could either define or compute the underlying cloud infrastructure network topology. This could be as simple as a YAML file that defines an underlying InfiniBand interconnect 2-1 Fat Tree topology, or a more complex solution that utilizes network performance metrics to measure bandwidth and latency between disjoint nodes to build a weighted proximity graph. As it is possible that such network parameters could change due to other usage or to reconfiguration, a method of periodically monitoring and updating this proximity network using measurement tools such as PerfSonar \cite{hanemann2005perfsonar} may also help keep an effective proximity metric between hosts. With a metric, one can then apply a proximity scheduler to handle high performance virtual cluster allocation requests effectively and in a way that will be far more optimal than a simple round robin scheduling mechanism. The OpenStack private cloud IaaS framework is proposed for this effort, however, further development is needed to bring such a scheduling mechanism to fruition.  

The use of service level agreements (SLAs) within cloud infrastructure allocation is a well studied aspect \cite{serrano2013towards}.  It is also possible that certain SLAs could be incorporated into a private cloud infrastructure such as OpenStack to simultaneously guarantee performance for virtual clusters with the above defined methods while concurrently offering "classical" VM workload scheduling for HTC, big data, or other cloud usage models. This would provide the same user experience yet support diverse workloads and performance expectations as defined by a given SLA. As it is likely such performance-tuned cloud infrastructure deployments as proposed in this dissertation will likely still be used for traditional cloud workloads, it is advantageous to leverage SLAs in this way.   

 

%%%%%%---------------------------------------------------------------
\section{Chapter Summary}
\label{chap7sum}
%%%%%%---------------------------------------------------------------



In summary, we expect the combination of transparent huge pages, an RDMA-capable interconnect multiplexed in hardware for use by both guest and hosts, a high performant hypervisor, and post-copy migration and cloning mechanisms, to enable a novel architecture for high performance virtual clusters. These mechanisms, if implemented and properly managed within a performance oriented scheduling system, could help change how cloud infrastructure supports HPC and big data applications, where performance, usability, and reconfigurability all are available.  These migration and specialized environment support capabilities may also help enable new runtime systems for large scale scientific applications. 


%and combined with an efficient OS, such as Kitten \cite{pedretti2011kitten}, could help enable a new runtime system to support extreme scale distributed memory computations with in-situ analysis \cite{ahrens2010visualization, vishwanath2011toward} for large scale scientific systems. 




