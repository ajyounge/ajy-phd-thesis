%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Conclusion}
\label{chap:conc}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

With the advent of virtualization and the availability of virtual machines through cloud infrastructure, a paradigm shift in distributed systems has occured. Many services and applications once deployed on workstations, private servers, personal computers, and even some supercomputers have migrated to a cloud infrastructure. The reasons for this change are vast, however these reasons are not enough to support all computational challanges within such a virtualized infrastructure.

The use of tightly coupled, distributed memory applications common in High Performance Computing communities, has seen a number of problems and complications when deployed in virtualized infrasturcture.  While the reasons for this can be vast, many challanges  stem from the performance impact and overhead associated with virtualization, along with a lack of hardware necessary to support such concurrent environments. This dissertation looks to evaluate virtualization's ability to support mid-tier scientific HPC applications, and what needs to take place to make high performance virtualization a reality.

From the beginning, this dissertation proposes the advent of high performance virtual clusters to support a wide array of scientific computation, including mid-tier HPC applications, as well as a framework for building such an environment. This framework aims at identifying virtualization overhead and finding solutions and best practices with performant hypervisors, providing support for advanced accelerators and interconnects to enable new class of applications, and evaluating potential methods using benchmarks and real-world applications. 

\TODO{Write a bit more here}

Chapter \ref{chap:related} studied the related research necessary for defining not only the context for virtualization and cloud computing, but also virtual clusters, and their history through supercomputing.  Chapter \ref{chap:cloud2011} looked to study the applicability of various hypervisors for supporting common HPC workloads through the use of benchmarks from a single-node aspect.  This found challenges and some solutions to these workloads, and identified missing gaps that exist. 

Chapter \ref{chap:hpgc2014} started the investigation of the utility of GPUs to support mid-tier scientific applications using the Xen hypervisor. This chapter provided a proof-of-concept that with proper configuration and utilizing the latest in hardware support, GPU passthrough was possible and a viable model for supporting CUDA-enabled applications, a fast-growing application set. Chapter \ref{chap:cloud2014} provides an in-depth comparison of multiple hypervisors using the SHOC GPU benchmark suite, as well as GPU-enabled HPC applications. Here we discover our KVM implementation performs at near-native speeds and allows for effective GPU utilization. 

Chapter \ref{chap:mdsimulations} takes the lessons learned with KVM in GPU passthrough and adds in SR-IOV InfiniBand support, a critical tool for supporting tighly coupled distributed memory applications, to build a small virtual cluster. This environment supports two class-leading Molecular Dynamics simulations, LAMMPs and HOOMD-blue, and shows how both applications can not only perform at near-native speeds, but also leverage the latest HPC technologies such as GPUDirect for efficient GPU-to-GPU communication. This framework is also enveloped in an OpenStack environment.     

Chapter \ref{chap:future-work} is an introspective look at other advancements that can be made in virtualization to support high performance virtual clusters. Specifically, this chapter details the utility of virtual clusters backed with hugepages, added support for specialized live migration techniques leveraging high speed RDMA-capable interconnects, VM cloning for fast deployment of virtual clusters themselves, and scheduling considerations for integration of high performance virtual clusters in OpenStack.  


\TODO{Answer the research question, can we support HPC with virtual clusters? How could this help with big data convergence?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact}
\label{sec:impact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This dissertation has illustrated how virtualization can be used to support HPC applications using virtual clusters.  While the example applications used herein are in relation to Molecular Dynamics, it is anticipated that this work is also equally applicable to other fields including Astronomy, High Energy Physics, Bioinformatics, and computational chemistry, to name a few. It is also possible that such applications can scale with future infrastructure deployments, however further study will be necessary to confirm these assumptions.

The model for PCI passthrough may also be able to impact other hardware. First, this could include the Intel Xeon Phi (croprocessor models, not the new Knights Landing CPU), or some emerging FPGA implementations like the Stillwater Knowledge Processing Unit (KPU), a distributed data flow processor.  The PCI-Express has been updgraded in spec 4.0 to support larger I/O devices and accelerators with large power deliver on-bus, leading to the assumpting that, for at least commodity x86 systems, there could be an increase in device utilization. The caveot to this will be if the PCIE bus is abandoned or superseeded by other methods, such as System-on-Chip designs or Nvidia's NVLink effort.  While the HPC accelerator usage could very well wane in the wake of novel many core architectures such as Knights Landing, such movements have still yet to take place within the HPC community.  

As some of the advances described in the dissertation have already made their way to the OpenStack cloud platform. With that, it may be possible to build such a cloud infrastructure to run high performance virtual clusters at a larger scale. Applying this infrasturcture, along with high level experiment management and support services, could lead to a new national scale cyberinfraustructre deployment.  In time and with further devleopment, this could deployed within the NSF-funded XSEDE project today.  

