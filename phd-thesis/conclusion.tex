%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Conclusion}
\label{chap:conc}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

With the advent of virtualization and the availability of virtual machines through the use of cloud infrastructure, a paradigm shift in distributed systems has occurred. Many services and applications once deployed on workstations, private servers, personal computers, and even some supercomputers, have migrated to a cloud infrastructure. The reasons for this change toward using cloud infrastructure are vast, and include advantages such as increased application flexibility and scalability, the ability for providers to leverages economies of scale, and customized, on-demand user environments.  However, these reasons may not be enough to support all computational challenges within such a virtualized infrastructure.

One example where cloud infrastructure in insufficient is with the support for distributed memory applications. The use of tightly coupled, parallel tasks common in High Performance Computing communities has seen a number of problems and complications when deployed in virtualized infrastructure.  While the reasons for lack of integration can be numerous, many challenges stem from two aspects; the performance impact and overhead associated with virtualization, and the lack of hardware necessary to support tightly coupled concurrent tasks.  While virtualized cloud infrastructure may not be able to aid in all HPC related activities, it is possible that if these limitations are either overcome or mitigated, virtualization may be able to offer benefits to the overall HPC community. These benefits could include dynamic allocation, enhanced migration and data mangement capabilities, or even bursting capabilities for rare event simulations, to name a few.  

This dissertation looks to evaluate virtualization's ability to support mid-tier scientific HPC applications. From the beginning, this dissertation proposes the advent of high performance virtual clusters to support advanced scientific computation, including tightly coupled HPC applications.  This dissertation proposes a framework for building such an environment, which aims to identify virtualization overhead and finding solutions and best practices with performant hypervisors. In this also includes the efforts needed to support advanced accelerators and interconnects common in HPC environments, to enable new class of applications in virtualized infrastructure. The framework has built into it methodology for evaluating potential deployments as discussed throughout using benchmarks and real-world applications.  Furthermore, it is proposed to use the OpenStack IaaS project to encompass these components together in a unified private cloud architecture.  

%\TODO{Write a bit more here}

Chapter \ref{chap:related} studied the related research necessary for defining not only the context for virtualization and cloud computing, but also virtual clusters and their history through supercomputing.  Chapter \ref{chap:cloud2011} looked to study the applicability of various hypervisors for supporting common HPC workloads through the use of benchmarks from a single-node aspect.  This found challenges and some solutions to these workloads, and identified missing gaps that exist. 

Chapter \ref{chap:hpgc2014} started the investigation of the utility of GPUs to support mid-tier scientific applications using the Xen hypervisor. This chapter provided a proof-of-concept that with proper configuration by utilizing the latest in hardware support, GPU passthrough was possible and a viable model for supporting CUDA-enabled applications, a fast-growing application set. Chapter \ref{chap:cloud2014} provides an in-depth comparison of multiple hypervisors using the SHOC GPU benchmark suite, as well as GPU-enabled HPC applications. Here we discover our KVM implementation performs at near-native speeds and allows for effective GPU utilization. 

Chapter \ref{chap:mdsimulations} takes the lessons learned with KVM in GPU passthrough and adds in SR-IOV InfiniBand support, a critical tool for supporting tighly coupled distributed memory applications, to build a small virtual cluster. This environment supports two class-leading Molecular Dynamics simulations, LAMMPs and HOOMD-blue, and shows how both applications can not only perform at near-native speeds, but also leverage the latest HPC technologies such as GPUDirect for efficient GPU-to-GPU communication. %This framework is also enveloped in an OpenStack environment.     

Chapter \ref{chap:future-work} is an introspective look at other advancements that can be made in virtualization to support high performance virtual clusters. Specifically, this chapter details the utility of virtual clusters backed with hugepages, added support for specialized live migration techniques leveraging high speed RDMA-capable interconnects, VM cloning for fast deployment of virtual clusters themselves, and scheduling considerations for integration of high performance virtual clusters in OpenStack.  


Given the efforts of this dissertation, we must evaluate the original hypothesis.  Can virtualized infrastructure, leveraging the practices of high performance virtual clusters as defined herein, support mid-tier scientific computing endeavors? In the beginning of Chapter 3, looking at the base case when this research started, the answer was a murky \emph{no}.  When the advances made in hypervisors, the ability to leverage accelerators such as Nvidia GPUs in Chapter 4 and 5, coupled with the addition of a high performance, low-latency interconnect in Chapter 6, and leveraging advanced tuning and hypervisor configurations with KVM, answer to they hypothesis  has changed to a skeptical \emph{yes}.  This conjecture is made by evaluating the support for a common and convoluted HPC application set of various molecular dynamics applications and seeing performance overheads under 2\% compared to native, bare metal configurations.  While more work is needed to evaluate the potential for high performance virtual clusters to scale out, current scaling to several nodes does not show any barriers at this time.  



%\TODO{Answer the research question, can we support HPC with virtual clusters? How could this help with big data convergence?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact}
\label{sec:impact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This dissertation has illustrated how virtualization can be used to support HPC applications using virtual clusters.  While the example applications used herein are in relation to Molecular Dynamics, it is anticipated that this work is also equally applicable to other fields including Astronomy, High Energy Physics, Bioinformatics, and computational chemistry, to name a few. In this, it is also desirous to have these high performance virtual clusters also provide a feature-rich yet performance infrastructure to new and emerging big data science applications and platform services.  While further work will likely be necessary in storage and I/O efforts related to virtualization, the potential is available to meet the demands of a convergent infrastructure with virtualization.   

It is also possible that such applications can scale with future infrastructure deployments, however further study will be necessary to confirm these assumptions.

The model for PCI passthrough may also be able to impact other hardware. First, this could include the Intel Xeon Phi (croprocessor models, not the new Knights Landing CPU), or some emerging FPGA implementations like the Stillwater Knowledge Processing Unit (KPU), a distributed data flow processor.  The PCI-Express has been upgraded in spec 4.0 to support larger I/O devices and accelerators with large power deliver on-bus, leading to the assumptions that, for at least commodity x86 systems, there could be an increase in device utilization. The caveat to this will be if the PCIE bus is abandoned or superseded by other methods, such as System-on-Chip designs or Nvidia's NVLink effort.  While the HPC accelerator usage could very well wane in the wake of novel many core architectures such as Knights Landing, such movements have still yet to take place within the HPC community.  

As some of the advances described in the dissertation have already made their way to the OpenStack cloud platform. With that, it may be possible to build such a cloud infrastructure to run high performance virtual clusters at a larger scale. Applying this infrastructure, along with high level experiment management and support services, could lead to a new national scale cyberinfraustructre deployment.  In time and with further development, this could deployed within the NSF-funded XSEDE project.  

