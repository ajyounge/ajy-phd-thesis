%% Author: Andrew J. Younge
%% PhD Thesis/Project

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Conclusion}
\label{chap:conc}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

With the advent of virtualization and the availability of virtual machines through the use of cloud infrastructure, a paradigm shift in distributed systems has occurred. Many services and applications once deployed on workstations, private servers, personal computers, and even some supercomputers have migrated to a cloud infrastructure. The reasons for this  trend toward cloud infrastructure are vast, with advantages including increased application flexibility and scalability, the ability for providers to leverages economies of scale, and customized, on-demand user environments.  However, these reasons may not be enough to support all computational challenges within such a virtualized infrastructure.

One example where cloud infrastructure has historically been insufficient is with the support for distributed memory applications. The use of tightly coupled, parallel tasks common in High Performance Computing communities has seen a number of problems and complications when deployed in virtualized infrastructure.  While the reasons for lack of integration can be numerous, many challenges stem from two aspects, the performance impact and overhead associated with virtualization, and the lack of hardware necessary to support tightly coupled concurrent tasks.  While virtualized cloud infrastructure may not be able to aid in all HPC related activities, it is possible that if these limitations are either overcome or mitigated, virtualization can offer benefits to many in the HPC community. These benefits could include dynamic resource allocation, enhanced migration and data management capabilities, or even bursting capabilities for rare event simulations, to name a few.  

This dissertation looks to evaluate virtualization's ability to support mid-tier scientific HPC applications. From the beginning, this dissertation proposes the advent of high performance virtual clusters to support advanced scientific computation, including tightly coupled HPC applications.  This dissertation's framework for building such an environment aims to identify virtualization overhead and to find solutions and best practices with performant hypervisors. This effort also includes defining the methodology needed for supporting advanced accelerators and interconnects common in HPC environments in order to enable a new class of applications in virtualized infrastructure. The framework has built into it cases for evaluating potential deployments as discussed throughout using benchmarks and real-world applications.  Furthermore, it is proposed to use the OpenStack IaaS project to encompass these components together in a unified private cloud architecture.  

%\TODO{Write a bit more here}

Chapter \ref{chap:related} studied the related research necessary for defining not only the context for virtualization and cloud computing, but also virtual clusters and the history of supercomputing.  Chapter \ref{chap:cloud2011} looked to study the applicability of various hypervisors for supporting common HPC workloads through the use of benchmarks from a single-node aspect.  This found challenges and some solutions to these workloads, and identified missing gaps that exist. 

Chapter \ref{chap:hpgc2014} started the investigation of the utility of GPUs to support mid-tier scientific applications using the Xen hypervisor. This chapter provided a proof-of-concept that, with proper configuration by utilizing the latest in hardware support, GPU passthrough is a viable model for supporting CUDA-enabled applications, a fast-growing application set. Chapter \ref{chap:cloud2014} provides an in-depth comparison of multiple hypervisors using the SHOC GPU benchmark suite, as well as a few GPU-enabled HPC applications. Here we discover our KVM implementation performs at near-native speeds and allows for effective GPU utilization, even outperforming our previous work with the Xen hypervisor. 

Chapter \ref{chap:mdsimulations} takes the lessons learned with KVM in GPU passthrough and adds in SR-IOV InfiniBand support. This high speed, low latency interconnect represents a critical tool for supporting tightly coupled distributed memory applications. With this, a high performance virtual cluster is created. This environment supports two class-leading Molecular Dynamics simulations, LAMMPs and HOOMD-blue, and shows how both applications can not only perform at near-native speeds, but also leverage the latest HPC technologies, such as GPUDirect, for efficient GPU-to-GPU communication across distributed memory resources. %This framework is also enveloped in an OpenStack environment.     

Chapter \ref{chap:future-work} is an introspective look at other advancements that can be made in virtualization to support high performance virtual clusters. This chaper details the mechanisms whereby virtual clusters leverage PCI passthrough for added hardware and I/O support, along with the reduction of overhead through the use of huge pages.  With this, methods are outlined for advanced virtualization techniques, such as live migration leveraging high speed RDMA-capable interconnects, possibilities for moving VMs closer to data sources, VM cloning for fast deployment of virtual clusters themselves, and scheduling considerations for integration of high performance virtual clusters. These tools are details with the consideration of integration within the OpenStack IaaS cloud for private cyberinfrastructure deployments.


Given the efforts of this dissertation, we must evaluate the original hypothesis; can virtualized infrastructure, leveraging the practices of high performance virtual clusters as defined herein, support mid-tier scientific computing endeavors? In the beginning of Chapter 3, looking at the base case when this research started, the answer was a murky \emph{no}.  When considering the advances made in hypervisors, the ability to leverage accelerators such as Nvidia GPUs in Chapter 4 and 5, the addition of a high performance, low-latency interconnect in Chapter 6, and the advanced tuning and configuration with KVM, answer to the hypothesis changes to an optimistic but skeptical \emph{yes}.  This conjecture is made by evaluating the support for a common and convoluted HPC application set of various molecular dynamics applications and observing virtualization performance overhead under 2\% when compared to native, bare metal configurations.  

While more work is needed to evaluate the potential for high performance virtual clusters to scale out, the current body of research does not illuminate any architectural barriers at this time.  As such, we expect this work with KVM high performance virtualization to extend significantly beyond the current efforts described herein, perhaps up to supporting high end HPC applications at Petascale.  If Petaflop computing is possible within an on-demand cloud infrastructure, this may in fact had a drastic change on the way the community views and utilizes mid-range scientific computing. Perhaps more importantly, the availability of such high-end resources to current and future big data analytics software toolkits and services may also have a drastic impact on overall time-to-solution for big data problems. This convergence between HPC and big data analytics could have significant cost savings for large-scale supercomputing facilities, as it makes it possible to support such diverse workloads with a single, massively parallel, advanced capability hardware platform.  Furthermore, allowing simulation an analytics codes to run on the same advanced hardware platform can enable new research in in-situ workflow composition and orchestration, potentialy decreasing the overall experiment wall clock time, leading to scientific discoveries quicker and in greater frequency.



%\TODO{Answer the research question, can we support HPC with virtual clusters? How could this help with big data convergence?}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Impact}
\label{sec:impact}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This dissertation has illustrated how virtualization can be used to support HPC applications using virtual clusters.  While the example applications used herein are in relation to molecular dynamics simulations, it is anticipated that this work is also equally applicable to other fields including astronomy, high energy physics, bioinformatics, and computational chemistry, to name a few. In this, it is also desirous to have these high performance virtual clusters provide a feature-rich yet performance infrastructure to new and emerging big data science applications and platform services.  While further work will likely be necessary in storage and I/O efforts related to virtualization, the potential exists today to meet the demands of a convergent infrastructure with virtualization.   

It is also possible and likely that such applications can scale with future infrastructure deployments.  Next steps in this direction to scale out could see virtual cluster sizes increase to thousands of nodes. Virtualization efforts using the Palacios VMM were able to scale a Cray XT4 system to 4096 nodes, reporting only a ~5\% overhead \cite{lange2011minimal}. While so far we have seen slightly better results with KVM at smaller scales, it is hopeful that the same scaling may be possible with KVM on newer hardware supporting a more diverse set of user environments.  

The model for PCI passthrough illustrated throughout the dissertation may also be able to impact other hardware. First, this could include the Intel Xeon Phi (co-processor models, not the Knights Landing CPU), or some emerging FPGA implementations like the Stillwater Knowledge Processing Unit (KPU), a distributed data flow processor.  The PCI-Express SIG specification has been upgraded in version 4.0 to support larger I/O devices and accelerators with on-bus power delivery, leading to the assumptions that, for at least commodity x86 systems, there could be an increase in PCIE device utilization. The caveat to this will be if the PCIE bus is abandoned or superseded by other methods, such as System-on-Chip designs or Nvidia's NVLink effort \cite{agarwal2015unlocking}, where such new designs do not take into account IOMMU virtualization.  While HPC accelerator usage could very well wane in the wake of novel many-core architectures, such as Knights Landing CPUs \cite{hemmert2016trinity}, these movements have still yet to take hold within the HPC community.  

As some of the advances described in the dissertation have already made their way to the OpenStack cloud platform. It is expected that OpenStack's usage will only grow in both academic research and industry over time.  Considering this, it may be possible to build such a cloud infrastructure to run high performance virtual clusters at a larger scale. Applying this infrastructure, along with high level experiment management and support services, could lead to a new national scale cyberinfrastructure deployment.  In time and with further development, this could be deployed within the NSF-funded XSEDE project, for example. Perhaps, instead of having separated hardware for HPC systems like the XSEDE Stampede resource at UT Austin and a separate cloud infrastructure deployment for HTC computing and science gateways like IU's Jetstream, a single, unified high performance cloud infrastructure could be utilized to provide all such needs concurrently.  Finally, because the advantages provided by vitualizaton are only now integrating with HPC, new distributed computing paradigms and runtime systems may not yet have been realized.   

