%% Author: Andrew J. Younge
%% PhD Thesis

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Introduction}
\label{chap:intro}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
\label{sec:overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\TODO{Give quick very high level view. 

Provide insightful quote regarding scientific computation 

Breifly describe current state of Distributed Systems.

Introduce the importance and impact of HPC.

Introduce need/desire for flexible utility computing.
- list major virtualization advantages

Describe major differences between HPC and cloud

Define how a merger between HPC and Cloud computing is desired.

Introduce how Virtual Clusters, can do this.
- what is a virtual cluster? What is it not?
- Missing performance considerations.

}

For years visionaries in computer science have predicted the advent of utility-based computing.  This concept dates back to John McCarthy's vision stated at the MIT centennial celebrations in 1961.

\begin{quote}
``If computers of the kind I have advocated become the computers of the future, then computing may someday be organized as a public utility just as the telephone system is a public utility... The computer utility could become  the basis of a new and important industry.``
\end{quote}
 Only recently has the hardware and software become available to support the concept of utility computing on a large scale.

The concepts inspired by the notion of utility computing have combined with the requirements and standards of Web 2. 0 \cite{alexander2006wnw} to create Cloud computing \cite{buyya2008moc, foster2008cca, aboveTheClouds}.  Cloud computing is defined as, ``A large-scale distributed computing paradigm that is driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are delivered on demand to external customers over the Internet." This concept of Cloud computing is important to Distributed Systems because it represents a true paradigm shift \cite{kuhn1970structure} within the entire IT infrastructure.  Instead of adopting the in-house services, client-server model, and mainframes, Clouds push resources out into abstracted services hosted \textit{en masse} by larger organizations.  This concept of distributing resources is similar to many of the visions of the Internet itself, which is where the ``Clouds'' nomenclature originated, as many people depicted the internet as a big fluffy cloud one connects to.

%Should this go into related research?
%\TODO{Blend between HPC and Cloud here in this paragraph a lot better}
While Cloud computing is changing IT infrastructure, it also has had a drastic impact on Distributed Systems itself, which has a different evolution. Gone are the IBM Mainframes of the 1980's which dominated the enterprise landscape.  While some mainframes still exist, they are used only for batch related processing tasks and are relatively unused for scientific applications as they are inefficient at Floating Point Operations.  As such, they were replaced with Beowulf Clusters \cite{sterling2001beowulf} of the 90's and 00's. Anovelty of Supercomputing is that instead of just one large machine, many machines are connected together and used to achieve a common goal, thereby maximizing the overall speed of computation.  Clusters represent a more commodity-based supercomputer, where off the shelf CPUs are used instead of the highly customized and expensive processors in Supercomputers.  Supercomputers and Clusters are best suited for large scale applications such as particle physics, weather forecasting, climate research, molecular  modeling, bioinformatics, and physical simulations, to name a few.  These applications are often termed ``Grand Challenge" applications and represent the majority of scientific calculations done on large-scale Supercomputing resources today. However, there exists a notable gap between high end high perofrmance computing and supercomputers, and what is available on the common desktop. This mid-tier scientific computation is a fast growing field that struggles to efficiently harness distributed systems while hoping to minimize extensive devleopment efforts. As such, the notion of Cloud computing Infrasturcture and platform services has been seen as appealing to many researchers.  

%this is a big segway here, need more background information to frame

As more domain science turns to the aid of computational resources for conducting novel scientific endeavours, there is a continuing and growing need for national cyberinfrastructure initiatives to support an increasingly diverse set of scientific workloads. This growth can be seen not only in the number of computational resource requests (cite XSEDE findings) but also in the recent increase in accelerators and hybrid computing models capable of quickly providing additional resources \cite{vetter2011keeneland}.  

Historically, this diversity was separated into High Performance Computing (HPC) and High Throughput Computing (HTC).  With HTC, computational problems can be separated into independent tasks that can execute in a pleasingly parallel fashion, happily gaining any available resources and rarely require significant communication or synchronization between tasks. HPC often represents computational problems that require significant communication and coordination to effectively produce results, usually with the use of a communication protocol such as MPI \cite{mpi}.  Recently however, many big-data paradigms \cite{bigdata} have been introduced in distributed systems that represent new computational models for distributed computing, such as MapReduce \cite{MapReduce} and the corresponding Apache Big Data stack \cite{Jha2014apache}. Supporting these different distributed computational paradigms requires a flexible infrastructure capable of providing computational resources for all possible models in a fast and efficient manner. 

\TODO{Introduce virtual clusters here, related to beowulf clusters, hpc systems}

One such proposed solution is the use of virtual clusters \cite{foster2006virtual} to provide distinct, separated environments on similar resources using virtual machines. However, past concerns with virtualization have limited the adoption of virtual clusters in many large scale cyberinfrastructure deployments. This has largely been due to the overhead of virtualization, whereby many scientific computations have experienced a signficiant and notable degredation in performance.  In an ecosystem framiliarized with HPC systems where pformance is paramount, this has been an obstructive hurdle for many even mid-level scientific endeavors.

The notion that virtualization and Cloud infrastructure has been in the past characterized many times. One of the most prominient examples of this is the Department of Energy's Magellan Project \cite{www-magellan}, where by the Magellan Final Report \cite{MagellanFinal} states the following finding as a Key Finding:
  
\begin{quote}
``\textbf{Finding 2. Scientific applications with minimal communication and I/O are best suited for
clouds.}

We have used a range of application benchmarks and micro-benchmarks to understand the performance of scientific applications. Performance of tightly coupled applications running on virtualized clouds using commodity networks can be significantly lower than on clusters optimized for these workloads. This can be true at even mid-range computing scales. For example, we observed slowdowns of about 50x for PARATEC on the Amazon EC2 instances compared to Magellan bare-metal (non-virtualized) at 64 cores and about 7x slower at 1024 cores on Amazon Cluster Compute instances, the specialized high performance computing offering. As a result, current cloud systems are best suited for high-throughput, loosely coupled applications with modest data requirements.``
\end{quote}

These findings underscore how classical usage of virtualization in cloud infrastructure has serious performance issues when running tightly coupled distributed memory applications. Many of these performance limitations are sound, given the limitation of a number of virtualization overheads commonplace in 2011, including shadow page tables, slow emulated Ethernet drivers, experimental hypervisors, and a complete lack of sophisticated hardware concurrently was commonplace in supercomputers and clusters. 

However,virtualization itself is not fundamentally limited by the overhead that causes issues in running high performance computing applications. Recent improvements in performance as well the recent usability of accelerators and high speed, low latency interconnects, as demonstrated in this manuscript, have made virtual clusters a more viable solution.  Furthermore, virtualization could even bring enhanced usability and enable specialized runtime components to HPC resources, adding significant value to traditional HPC resources.  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Statement}
\label{sec:stmt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The open question is if Cloud infrastructure, using virtualizaton, can support mid-tier scientific computation.  This argument is that yes, it potentially can.

With the rise of cloud computing within the greater realm of distributed systems, we've seen a number of scientific computing endeavors map well to cloud infrastructure. First, this includes the simple and most common practice of on demand computing, where by scientists can rent-a-workstation \cite{something-boring}, perhaps more powerful than a given researcher's laptop, to run their scientific applications on or support greater laboratory collaborative efforts, such as a shared database or web service. Second, we've seen virtualized cloud infrastructure support high throughput computing very well. Often times pleasingly parallel applications, be it from high energy physics such as the LHC effort \cite{lhc, cern-openstack} or bioinformatics with BLAST alignment jobs \cite{cloud-blast}, have proven to run with high efficiency in public and private cloud environments. The rise of public cloud infrastructure has also coincided with increase in big data computation and analytics. \TODO{give more info on Big data here}.  Many of these big data platform services have evolved complimentary to cloud infrastructure, and as such are well support with virtualization \cite{hadoop-ec2}.  However, often times in the past tightly coupled, high performance distributed memory applications that often support leading class scientific efforts run very poorly on virtualized infrastructure.  While this is due to a myriad of addressable reasons ranging from scheduling, abstraction overhead, or a lack of advanced hardware support, it is an open question on weather virtualization can in fact support such tightly coupled large scale applications without an imposed significant performance penalty.  

\TODO{Talk about virtual clusters, what they are, how they help support diverse workloads}

\TODO{Discuss cloud vs hpc}

Given the current outlook, this work proposes a comprehensive framework for supporting High Performance Virtual Clusters that enable advanced computational workloads, including tightly coupled distributed memory applications, using virtualization technologies. This framework, outlined in Figure \ref{F:framework}, illustrates the topics that must be addressed to provide a supportive virtual cluster environment 

 \FIGURE{htb}
  {images/VirtualClusterFramework.pdf}
  {1.0}
  {High Performance Virtual Cluster Framework}
  {F:framework}

In order to provide comprehensive high performance virtual clusters, we need to first consider the most important aspect, the virtualized infrastructure itself. At the core, we have to consider the hypervisor or virtual machine monitor and the overhead and performance characteristics of it. This includes performance tuning considerations, NUMA effects, and advanced hardware device passthrough. Specifically, device passthrough in the context of this manuscript refers to two major concentrations: GPUs and InfiniBand interconnects (the later using SR-IOV). The virtual infrastructure also must consider scheduling as a major factor in performing efficient placement of workloads on the underlying host infrastructure, and in particular a Proximity scheduler is of interest. Storage solutions in a virtualized environment is an increasingly important aspect of this framework, as both HPC and big data solutions are continuing to prioritize I/O performance compared to computation. Storage is also likely to be heavily dependent on interconnect considerations as well, as potentially provided by device passthrough.  

However, providing an enhanced virtualized infrastructure does not solve all aspects of high performance virtual clusters. Specifically, proposed infrasturctures need to be properly evaluated in a systematic way through the use of a wide array of benchmarks, mini-applications, and full scale scientific applications. This effort can further be broken separated into three major problem sets; base level benchmarking tools, HPC applications, and big data applications. Evaluting the stringent performance requirements of all three sets, when compared with bare metal (no virtualization) common solutions, will illuminate not only successful designs but also the necessary focus areas that require more attention.

One of the higher level aspects of providing high performance virtual clusters is the high level orchestration of the virtual clusters themselves. While this largely remains tangential to this research, it is none the less a key aspect for a successful soltuion. Some effort has been put forth for virtual cluster experiment management \cite{futureGrid-experiment-management}, and many ongoing open sources solutions also offer compelling options, such as OpenStack Heat \cite{www-openstack-heat}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Challenges}
\label{sec:chall}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Historically, 

The utility of Cloud computing

\TODO{ Also bring in the Magellan project's EC2 paper here, show what they were doing, how performance is a limiting factor}

- Quickly state our desires: support HPC and big data
- State our solution is ot use virtualization
- However, historically virtualization has performance issues



- Performance must be a first class function
- Need for a framework.  to orgnaize efforts
- Need to address not only HPC but also big data
- HPC and Big data convergence 


\subsection{Contributions}


 \FIGURE{htb}
  {images/VirtualCluster-architecture.pdf}
  {1.0}
  {Architectural diagram for High Performance Virtual Clusters}
  {F:hpvcarch}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}
\label{sec:outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The rest of this dissertation is organized into chapters, each signifying the steps to move forward the notion of a high performance virtual cluster solution.

Chapter \ref{chap:related} investigates the related research surrounding both cloud computing and high performance computing. Within cloud computing, an introduction to cloud infrastructure, virtualization, and containers will all be discussed. This also includes details regarding virtual clusters as well as an overview of some national scale cloud infrastructure efforts that exist. Furtermore, we investigate the state of high performance computing and supercomputing, as well touch upon some of the current exascale efforts.

Chapter \ref{chap:cloud2011} takes a look at the potential for virtualization, in its current state, for high performance computing. This includes a feature comparison for hardware availability of a few common hypervisors, specifically Xen, KVM, VirtualBox, and VMWare. Then, a few common HPC benchmarks are evaluated to determine what overhead exists and where in a single node configuration. This identifies how in some senarioes, virtualization adds only a minor overhead, whereas with other scenarios, overheads can be up to 50\% compared to native configurations. 

Chapter \ref{chap:hpgc2014} starts to overcome one of the main limitations of virtualization for use in advaced scientific computing, specifically the lack of hardware avaiability. In this chaper, The Xen hypervisor is used to demonstrate the first publicized affect of GPU Passthrough, allowing for GPUs to be used in a guest VM. The efficiency of this method is breifly evaluated using two different hardware setups, and finds hardware can play a notable role in single node performance. 

Chapter \ref{chap;cloud2014} continues where chapter \ref{chap:hpgc2014} leaves off, by demonstrating that GPU passthrough is possible on many other hypervisors, specifically also KVM and VMWare, and compares with one of the main containerization solutions, LXC. Here, the GPUs are evaluated using not only the HPC SHOC GPU benchmark suite developed at Oak Ridge National Laboratory, but also a diverse mix of real-world applications to examine how and where overhead exists with GPUs in VMs for each virtualization setup.  Specifically, we find that with properly tuned hardware and NUMA-balanced configurations, that the KVM solution can perform at roughly near-native performance, with on average ~1.5\% overhead compared to no virtualization. This illustrates that with the combination correct hypervisor selection, careful tuning, and advanced hardware, scientific computations can be supported using virtualized hardware. 

Chapter \ref{chap:mdsimulations} takes the findings from the previous chapter to the next level. Specifically, the lessons learned from successful KVM virtualization with GPUs is expanded and combined with a missing key component of supporting advanced parallel computations: a high speed, low latency interconnect, specifically InfiniBand. Using SR-IOV and PCI Passthrough of QDR InfiniBand interconnect across a small cluster, it is demonstrated that two Molcular Dynamics simulations, both very commonly used in the HPC community, can be run at near-native performance in the designed virtual cluster.

Chapter \ref{chap:future-work} takes a look at the given situation, and puts forth an argument for a high performance virtual cluster solutions. Specifically, we look at the given state of the art, how virtual clusters can be used to provide an infrastructure to support the convergence between HPC and big data. Furthermore, this chapter outlines and investigates potential next steps for virtualization, including the potential for advanced live migration techniques and VM cloning, which can be made available with the inclusion of a high-performance RDMA-capable interconnect. 


Finally, this work concludes with an overal view of the current state of high performance virtualization, as well as it's potential to impact and support a wide array of disciplines. 



