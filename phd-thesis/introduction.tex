%% Author: Andrew J. Younge
%% PhD Thesis

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Introduction}
\label{chap:intro}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
\label{sec:overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\TODO{Give quick very high level view. 

Provide insightful quote regarding scientific computation 

Breifly describe current state of Distributed Systems.

Introduce the importance and impact of HPC.

Introduce need/desire for flexible utility computing.
- list major virtualization advantages

Describe major differences between HPC and cloud

Define how a merger between HPC and Cloud computing is desired.

Introduce how Virtual Clusters, can do this.
- what is a virtual cluster? What is it not?
- Missing performance considerations.

}
\end{comment}


For years visionaries in computer science have predicted the advent of utility-based computing.  This concept dates back to John McCarthy's vision stated at the MIT centennial celebrations in 1961.

\begin{quote}
``If computers of the kind I have advocated become the computers of the future, then computing may someday be organized as a public utility just as the telephone system is a public utility... The computer utility could become  the basis of a new and important industry.``
\end{quote}
 Only recently has the hardware and software become available to support the concept of utility computing on a large scale.

The concepts inspired by the notion of utility computing have combined with the requirements and standards of Web 2.0 \cite{alexander2006wnw} to create Cloud computing \cite{buyya2008moc, foster2008cca, aboveTheClouds}.  Cloud computing is defined as, ``A large-scale distributed computing paradigm that is driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are delivered on demand to external customers over the Internet." This concept of cloud computing is important to Distributed Systems because it represents a true paradigm shift \cite{kuhn1970structure} within the entire IT infrastructure.  Instead of adopting the in-house services, client-server model, and mainframes, clouds push resources out into abstracted services hosted \textit{en masse} by larger organizations.  This concept of distributing resources is similar to many of the visions of the Internet itself, which is where the ``clouds'' nomenclature originated, as many people depicted the internet as a big fluffy cloud one connects to.

As the core of most cloud infrastructure lies virtualization, a computer architecture technology by which 1 or more Virtual Machines (VMs) are run on the same physical host. In doing this, a layer of abstraction is inserted between and around the hardware and Operating System (OS). Specifically, hardware resources such as CPUs, memory, and I/O devices, and software resources analagous to OS functionality and low level libraries are abstracted and provided to VMs directly. While virtualization has existed for many years,  its availability with Intel x86 commodity hardware in conjunction with the rise of clouds has brought virtualization to the forefront of distributed systems. 

%Should this go into related research?
%\TODO{Blend between HPC and Cloud here in this paragraph a lot better}
While cloud computing is changing IT infrastructure, it also has had a drastic impact on distributed systems itself, which has a different evolution. Gone are the IBM Mainframes of the 1980's which dominated the enterprise landscape.  While some mainframes still exist, they are used only for batch related processing tasks and are relatively unused for scientific applications as they are inefficient at Floating Point Operations.  As such, they were replaced with Beowulf Clusters \cite{sterling2001beowulf}, Massively Parallel Processors (MPPs) and Supercomputers of the 90's and 00's. A novelty of these distributed memory systems is that instead of just one large machine, many machines are connected together and used to achieve a common goal, thereby maximizing the overall speed of computation.  Clusters represent a more commodity-based supercomputer, where off the shelf CPUs are used instead of the highly customized and expensive processors and interconnects found in Supercomputers.  

Supercomputers and Clusters are best suited for large scale applications.  These HPC applications can even include ``Grand Challenge" applications \cite{hoare2005grand} and can represent a sizable amount of the scientific calculations done on large-scale Supercomputing resources today. However, there exists a gap of many orders of magnitude  between leading-class high performance computing and what is available on the common laboratory workshop. This gap, described here as mid-tier scientific computation is a fast growing field that struggles to efficiently harness distributed systems while hoping to minimize extensive development efforts. These mid-tier scientific endeavours need to leverage distributed systems to effectively complete the calculations at hand, however may not require the extreme scale provided by the latest machines at the peak of the Top500 list \cite{www-top500}. It simply may not be feasible for small research teams to effectively handle the development complexity of extreme-scale resources, and instead look towards other options.  This can include some scientific disciplines such as high energy physics \cite{buncic2010cernvm}, materials science \cite{wang2006survey}, bioinformatics \cite{menon2012cloud}, and climate research \cite{He2010nasa}, to name a few.  


As more domain science turns to the aid of computational resources for conducting novel scientific endeavours, there is a continuing and growing need for national cyberinfrastructure initiatives to support an increasingly diverse set of scientific workloads. Substantial growth can be see in the number of computational resource requests \cite{towns2014xsede, antypas2008nersc} from many of the larger computational facilities.  Concurrently, there has also been an increase in accelerators and hybrid computing models capable of quickly providing additional resources \cite{vetter2011keeneland} beyond commodity clusters.

Historically, application diversity was separated into High Performance Computing (HPC) and High Throughput Computing (HTC).  With HTC, computational problems can be separated into independent tasks that can execute in a pleasingly parallel fashion, happily gaining any available resources and rarely require significant communication or synchronization between tasks. HPC often represents computational problems that require significant communication and coordination to effectively produce results, usually with the use of a communication protocol such as MPI \cite{mpi}.  Recently however, many big-data paradigms \cite{agrawal2011big} have been introduced in distributed systems that represent new computational models for distributed computing, such as MapReduce \cite{dean2008mapreduce} and the corresponding Apache Big Data stack \cite{kamburugamuve2013survey, chen2014big}. Supporting these different distributed computational paradigms requires a flexible infrastructure capable of providing computational resources for all possible models in a fast and efficient manner.



Currently we are at the forefront of a convergence within scientific computing between HPC and big data computation \cite{reed2015exascale}. This amalgamation of historically differing viewpoints of Distributed Systems looks to combine the performance characteristics of HPC and the pursuit towards Exascale with the data and programmer oriented concurrency models found in Big Data and cloud services. 

Much of the convergence effort has been focused on applications and platform services. Specifically, significant work towards convergent applications has been outlined with the Big Data Ogres \cite{Jha2014apache} and the corresponding HPC-ABDS model \cite{qiu2014towards}.  This convergence can also be seen with efforts in bringing interconnect advances to classically big data platforms such as with InfiniBand and MapReduce \cite{panda2013hadoop}. However, the underlying harward and OS environments are still something to be reconciled, and represents something that virtualization can potentially provide. It is expected that new big data efforts will continue to move in this direction \cite{ekanayake2016spidal}, especially if virtualization can make HPC hardware that's traditionally prohibitive in such areas, such as accelerators and high-speed interconnects, readily available to cloud and big data platforms. As the deployment of big data applications and platform services on virtualized infrastructure is well defined and studied \cite{tian2011towards}, this dissertation has focused the difficulty of running HPC applications on similar virtualized infrastructure.  However, it is possible and hopeful that research regarding virtualization can also play a part in bringing advanced hardware and performance-focused considerations to Big Data applications, effectively cross-cutting the convergence with HPC. In Summary, success of the research in virtualization could be defined by the ability to support the convergence between HPC and Big Data.

\FIGURE{htb}
 {images/convergent-ecosystem.pdf}
 {1.0}
 {Data analytics and computing ecosystem compared (from \cite{reed2015exascale}), with virtualization included}
 {F:convergence}


To further illustrate where virtualization can play a part in HPC and Big Data convergence, we look at Figure \ref{F:convergence} from Reed \& Dongarra \cite{reed2015exascale}. While the two ecosystems depicted are only representative and in no way exhaustive, they do show how drastically different user environments exist and are reliant on differing hardware. If we insert a performance-oriented virtualization mechanism within the system software capable of handling the advanced cluster hardware and perform at near-native speeds (at or under 5\% overhead, as loosely defined in \cite{lange2010palacios}), it could provide a single, comprehensive \emph{convergent ecosystem} that supports both HPC and Big Data efforts at a critical level. 


This work proposes the use of virtual clusters \cite{Foster2006} to provide distinct, separated environments on similar resources using virtual machines. These virtual clusters, similar in design to the Beowulf clusters and commodity HPC systems, provide a distributed memory environment, usually with a local resource management system \cite{czajkowski1998resource}.  However, past concerns with virtualization have limited the adoption of virtual clusters in many large scale cyberinfrastructure deployments. This has largely been due to the overhead of virtualization, whereby many scientific computations have experienced a significant and notable degradation in performance.  In an ecosystem familiarized with HPC systems where performance is paramount, this has been an obstructive hurdle for deploying many tightly coupled applications. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Statement}
\label{sec:stmt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\TODO{The open question is if Cloud infrastructure, using virtualizaton, can support mid-tier scientific computation.  This argument is that yes, it potentially can.}

With the rise of cloud computing within the greater realm of distributed systems, there have been a number of scientific computing endeavors map well to cloud infrastructure. This first includes the simple and most common practice of on demand computing where by users can rent-a-workstation \cite{kondo2009cost}. Perhaps these resources are more powerful than a given researcher's laptop and used to run their scientific applications or support greater laboratory collaborative efforts, such as a shared database or Web services.  We've also seen virtualized cloud infrastructure support high throughput computing very well. Often times pleasingly parallel applications, be it from high energy physics such as the LHC effort \cite{buncic2010cernvm, bell2015scaling} or bioinformatics with BLAST alignment jobs \cite{menon2012cloud}, have proven to run with high efficiency in public and private cloud environments. Furthermore, the rise of public cloud infrastructure has also coincided with increase in big data computation and analytics.  Many of these big data platform services have evolved complimentary to cloud infrastructure, and as such have a symbiotic relationship with virtualization technologies \cite{gunarathne2010mapreduce}.  

However, with tightly coupled, high performance distributed memory applications, the same endeavors that support leading class scientific efforts, run very poorly on virtualized cloud infrastructure \cite{ostermann2009performance}.  This is due to a myriad of addressable reasons ranging from scheduling, abstraction overhead, or a lack of advanced hardware support necessary for tightly coupled communication. This postulates the question on whether virtualization can in fact support such tightly coupled large scale applications without an imposed significant performance penalty. Simply put, \emph{the goal of this dissertation is to investigate the viability of mid-tier scientific applications supported in virtualized infrastructure}.  

Historically, mid-tier scientific applications are distributed memory HPC applications that require more complex process communication mechanisms. These systems need far more performance than a single compute resources (such as a workstation) can provide. This could include hundreds or thousands of processes calculating and communicating concurrently on a cluster, perhaps using a messaging interface such as MPI.  These applications are likely distinct either in application composition or operating perameters, from extreme-scale HPC applications that run at the highest end of the supercomputing resources today operating on petascale machines and beyond.  


%\TODO{Mention virtual clusters quickly here}

%\TODO{Discuss cloud vs hpc}

Given the current outlook on virtualization for supporting HPC applications, this dissertation proposes a framework for High Performance Virtual Clusters that enable advanced computational workloads, including tightly coupled distributed memory applications, to run with a high degree of efficiency  in virtualized environments. This framework, outlined in Figure \ref{F:framework}, illustrates the topics to be addressed to provide a supportive virtual cluster environment for high performance mid-tier scientific applications.  Areas marked in darker green indicate topics this dissertation may touch upon, whereas light green areas in Figure \ref{F:framework} identify outstanding considerations to be investigated. We specifically identify mid-tier distributed memory parallel computations as a focal point for the computational challenges at hand as a way to separate from some of the latest efforts in towards Exascale \cite{dongarra2011exascale, bergman2008exascale, shalf2010exascale} computing.  While virtualization may in fact be able to play a role towards usable Exascale computing, such efforts fall outside the immediate scope of this dissertation. 


 \FIGURE{htb}
  {images/VirtualClusterFramework.pdf}
  {1.0}
  {High Performance Virtual Cluster Framework}
  {F:framework}

In order to provide comprehensive high performance virtual clusters, we need to first look at a key area itself, the virtualized infrastructure itself. At the core, we have to consider the hypervisor or virtual machine monitor and the overhead and performance characteristics associated with it. This includes performance tuning considerations, NUMA effects, and advanced hardware device passthrough. Specifically, device passthrough in the context of this manuscript refers to two major device types; GPUs and InfiniBand interconnects (the later using SR-IOV). The virtual infrastructure also must consider scheduling as a major factor in performing efficient placement of workloads on the underlying host infrastructure, and in particular a Proximity scheduler is of interest \cite{www-proximity-scheduler}. Storage solutions in a virtualized environment is an increasingly important aspect of this framework, as both HPC and big data solutions are continuing to prioritize I/O performance compared to computation. Storage is also likely to be heavily dependent on interconnect considerations as well, as potentially provided by device passthrough, however such I/O considerations lie beyond this dissertations immediate scope.  

However, simply providing an enhanced virtualized infrastructure may not guarantee that all implementations of high performance virtual clusters are performant. Specifically, proposed infrastructures need to be properly evaluated in a systematic way through the use of a wide array of benchmarks, mini-applications, and full-scale scientific applications. This effort can further be broken separated into three major problem sets; base level benchmarking tools, HPC applications, and big data applications. Evaluating the stringent performance requirements of all three sets, when compared with bare metal (no virtualization) solutions, will illuminate not only successful designs but also the focus areas that require more attention.  As such, we look to continually use these benchmarks and applications as a tool to measure the viability of virtualization in this context. 

 \FIGURE{htb}
  {images/VirtualCluster-architecture.pdf}
  {1.0}
  {Architectural diagram for High Performance Virtual Clusters}
  {F:hpvcarch}

Building form the historical virtual clusters in Grid computing, we see a new architectural model for high performance virtual clusters illustrated in \ref{F:hpvcarch} that is implied by this dissertation.  Here, we leverage commodity hardware, as well as some advanced HPC hardware. While this notion could incorporate a wide array of differing technologies, we focus here on GPU-based accelerators and InfiniBand interconnects in conjunction with x86 CPUs. Atop this, we leverage KVM and QEMU to provide an advanced hypervisor for creating and hosting VMs with direct hardware involvement from the lower level. Moving up, the Libvirt API is leveraged due to its hypervisor interoperability and popularity. Atop Libvirt is the OpenStack private cloud infrastructure. In Figure \ref{F:hpvcarch} we illustate some (but not all) of OpenStack's services including the Horizon UI, Cinder and Glance storage mechanisms, and the Neutron (previously Quantum) components. The Nova component of OpenStack is the point of focus for providing comprehensive VM management.
\footnote{While many of the features for nova's additions in GPU Passthrough and SR-IOV InfiniBand support have been put together at USC/ISI as an OpenStack Nova fork (https://libraries.io/github/usc-isi/nova), much of the features have since been modified and matured by the OpenStack community in later releases and made available in upstream Nova.}  Atop OpenStack, we can create a wide array of virtual clusters and machines to support the wide ranging scientific computing ecosystems necessary. This includes application models ranging from tightly coupled MPI+CUDA HPC applications, to emerging big data analytics toolkits such as Apache Storm \cite{kamburugamuve2016streaming}. 

One of the higher-level aspects of providing high performance virtual clusters is the high level orchestration of the virtual clusters themselves, which we term experiment management. While this largely remains tangential to this immediate research, it is none the less a key aspect for a successful solution. Some effort has been put forth for virtual cluster experiment management \cite{las2010gce}, and many ongoing open sources solutions also offer compelling options, such as OpenStack Heat \cite{www-openstack-heat}.  An example of a project delivering advanced orchestration mechanisms and a toolkit to aid in configurable virtual clusters on heterogeneous IaaS deployments is the Cloudmesh effort \cite{von2014cloudmesh}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Challenges}
\label{sec:chall}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The framework, architecture, and efforts described in this dissertation represent a movement forward in providing virtualized infrastructure to support a wide arrange of scientific applications. However, there still exist some challenges that will need to be addressed.  This includes a sigma of virtualization being inherently slow and unable to support tightly coupled computations, limitations with advancing at scale, and even that containers may provide a better alternative.  While this work hopes to move beyond these challenges, they none the less must be considered. 

The notion that virtualization and Cloud infrastructure are not able to support parallel distributed memory applications has been characterized many times. One of the most prominent examples of this is the Department of Energy's Magellan Project \cite{www-magellan}, where by the Magellan Final Report \cite{MagellanFinal} states the following finding as a Key Finding:
  
\begin{quote}
``\textbf{Finding 2. Scientific applications with minimal communication and I/O are best suited for clouds.}

We have used a range of application benchmarks and micro-benchmarks to understand the performance of scientific applications. Performance of tightly coupled applications running on virtualized clouds using commodity networks can be significantly lower than on clusters optimized for these workloads. This can be true at even mid-range computing scales. For example, we observed slowdowns of about 50x for PARATEC on the Amazon EC2 instances compared to Magellan bare-metal (non-virtualized) at 64 cores and about 7x slower at 1024 cores on Amazon Cluster Compute instances, the specialized high performance computing offering. As a result, current cloud systems are best suited for high-throughput, loosely coupled applications with modest data requirements.``
\end{quote}

These findings underscore how classical usage of virtualization in cloud infrastructure has serious performance issues when running tightly coupled distributed memory applications. Many of these performance concerns are sound, given the limitation of a number of virtualization overheads commonplace at the time, including shadow page tables, emulated Ethernet drivers, experimental hypervisors, and a complete lack of sophisticated hardware commonplace in supercomputers and clusters.  As a result, the advantages of virtualization, including on-demand resource allocation, live migration and advanced hybrid migration, and user-defined environments, have not been able to effectively show their value in the context of the HPC community.

Other and related efforts within the scientific community too found limitations with HPC applications in public cloud environments. This includes the study by Jackson et. al \cite{jackson2010performance} which illustrates how the Amazon Elastic Compute Cloud (EC2) creates a 6x performance impact compared to a local cluster, due to a large part on the limiting Gigabit Ethernet that benchmarks relied heavily on within the EC2 system. Other studies also found similar results such as Ostermann \cite{ostermann2009performance}, concluding that Amazon EC2 ``is insufficient for scientific computing at large, though it still appeals to the scientists that need resources immediately and temporarily."  However, these studies are now outdated and do not take into account the advancements in virtualization and hardware availability detailed in this dissertation. Specifically, it is estimated that with the KVM hypervisor in a performance-tuned environment, using accelerators and most certainly a high-speed, low latency interconnect as detailed in Chapter \ref{chap:mdsimulations}, the results would be drastically different. 


One limitation in this research in high performance virtual clusters is the fact that thedegree to which applications can scale remains relatively unknown. While initial results with SR-IOV InfiniBand are promising, scaling is naturally hard to predict. While unfounded, it would be hypothetically possible that as the number of VM's increases, tail-latency could also increase, causing notable slowdowns during distributed memory synchronization barriers. It is only when infrastructure comes available to support high performance virtual clusters will scaling beyond to thousands of cores and beyond be investigated.  

Another potential challenge, and perhaps also a strength, is the rising use of containers within industry, such as with efforts like Docker \cite{merkel2014docker}. Recently, we've seen efforts at NERSC/LBNL adapt a container solution called Shifter with the SLURM resource manager on CRAY XC based systems \cite{jacobsen2015contain}. Shifter's goal is to provide user defined images for NERSC's bioinformatics users, and it adapts remarkably well to the HPC environment. While further efforts are needed by Cray and NERSC to fully provide a container-bases solution on a large scale Supercomputer for all applications, its efforts are in many ways parallel to using virtualization. In Chapter \ref{chap:cloud2014}, we specifically compare virtualization efforts with LXC \cite{xavier2013performance}, a popular Linux container solution and find performance to be comparable and largely near-native. 

 
While the major concern with virtualization in the HPC community is performance issues, virtualization itself may not be fundamentally limited by the overhead that causes issues in running high performance computing applications. Recent improvements in performance, along with increased usability of accelerators and high speed, low latency interconnects in virtualized environments as demonstrated in this dissertation, have made virtual clusters a more viable solution for mid-tier scientific applications.  Furthermore, it is possible for  virtualization technologies to bring enhanced usability and enable specialized runtime components to future HPC resources, adding significant value over today's supercomputing resources.  This could potentially include infrastructure advances for higher level cloud platform services for supporting big data applications \cite{qiu2014towards}. 




%\subsection{Contributions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}
\label{sec:outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The rest of this dissertation is organized into chapters, each signifying the steps to move forward the notion of a high performance virtual cluster solution.

Chapter \ref{chap:related} investigates the related research surrounding both cloud computing and high performance computing. Within cloud computing, an introduction to cloud infrastructure, virtualization, and containers will all be discussed. This also includes details regarding virtual clusters as well as an overview of some national scale cloud infrastructure efforts that exist. Furthermore, we investigate the state of high performance computing and supercomputing, as well touch upon some of the current Exascale efforts.

Chapter \ref{chap:cloud2011} takes a look at the potential for virtualization, in a base case, for high performance computing. This includes a feature comparison for hardware availability of a few common hypervisors, specifically Xen, KVM, VirtualBox, and VMWare. Then, a few common HPC benchmarks are evaluated to determine what overhead exists and where in a single node configuration. This identifies how in some scenarios, virtualization adds only a minor overhead, whereas with other scenarios, overheads can be up to 50\% compared to native configurations. 

Chapter \ref{chap:hpgc2014} starts to overcome one of the main limitations of virtualization for use in advanced scientific computing, specifically the lack of hardware availability. In this chapter, The Xen hypervisor is used to demonstrate the affect of GPU Passthrough, allowing for GPUs to be used in a guest VM. The efficiency of this method is briefly evaluated using two different hardware setups, and finds hardware can play a notable role in single node performance. 

Chapter \ref{chap:cloud2014} continues where chapter \ref{chap:hpgc2014} leaves off, by demonstrating that GPU passthrough is possible on many other hypervisors, specifically also KVM and VMWare, and compares with one of the main containerization solutions, LXC. Here, the GPUs are evaluated using not only the SHOC GPU benchmark suite developed at Oak Ridge National Laboratory, but also a diverse mix of real-world applications to examine how and where overhead exists with GPUs in VMs for each virtualization setup.  Specifically, we find that with properly tuned hardware and NUMA-balanced configurations, that the KVM solution can perform at roughly near-native performance, with on average ~1.5\% overhead compared to no virtualization. This illustrates that with the combination correct hypervisor selection, careful tuning, and advanced hardware, scientific computations can be supported using virtualized hardware. 

Chapter \ref{chap:mdsimulations} takes the findings from the previous chapter to the next level. Specifically, the lessons learned from successful KVM virtualization with GPUs is expanded and combined with a missing key component of supporting advanced parallel computations: a high speed, low latency interconnect, specifically InfiniBand. Using SR-IOV and PCI passthrough of QDR InfiniBand interconnect across a small cluster, it is demonstrated that two Molecular Dynamics simulations, both very commonly used in the HPC community, can be run at near-native performance in the designed virtual cluster.

Chapter \ref{chap:future-work} takes a look at the given situation of virtualization, and puts forth an argument for enhancements forthcoming in high performance virtual cluster solutions. Specifically, we look at the given state of the art, how virtual clusters can be used to provide an infrastructure to support the convergence between HPC and big data. Specifically, this chapter outlines and investigates potential next steps for virtualization, including the potential for advanced live migration techniques and VM cloning, which can be made available with the inclusion of a high-performance RDMA-capable interconnect. 

Finally, this work concludes with an overall view of the current state of high performance virtualization, as well as it's potential to impact and support a wide array of disciplines. 



