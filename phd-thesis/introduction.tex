%% Author: Andrew J. Younge
%% PhD Thesis

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Introduction}
\label{chap:intro}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
\label{sec:overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\TODO{Give quick very high level view. 

Provide insightful quote regarding scientific computation 

Breifly describe current state of Distributed Systems.

Introduce the importance and impact of HPC.

Introduce need/desire for flexible utility computing.
- list major virtualization advantages

Describe major differences between HPC and cloud

Define how a merger between HPC and Cloud computing is desired.

Introduce how Virtual Clusters, can do this.
- what is a virtual cluster? What is it not?
- Missing performance considerations.

}
\end{comment}


For years visionaries in computer science have predicted the advent of utility-based computing.  This concept dates back to John McCarthy's vision stated at the MIT centennial celebrations in 1961.

\begin{quote}
``If computers of the kind I have advocated become the computers of the future, then computing may someday be organized as a public utility just as the telephone system is a public utility... The computer utility could become  the basis of a new and important industry.``
\end{quote}
 Only recently has the hardware and software become available to support the concept of utility computing on a large scale.

The concepts inspired by the notion of utility computing have combined with the requirements and standards of Web 2.0 \cite{alexander2006wnw} to create Cloud computing \cite{buyya2008moc, foster2008cca, aboveTheClouds}.  Cloud computing is defined as, ``A large-scale distributed computing paradigm that is driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are delivered on demand to external customers over the Internet." This concept of Cloud computing is important to Distributed Systems because it represents a true paradigm shift \cite{kuhn1970structure} within the entire IT infrastructure.  Instead of adopting the in-house services, client-server model, and mainframes, Clouds push resources out into abstracted services hosted \textit{en masse} by larger organizations.  This concept of distributing resources is similar to many of the visions of the Internet itself, which is where the ``Clouds'' nomenclature originated, as many people depicted the internet as a big fluffy cloud one connects to.

%Should this go into related research?
%\TODO{Blend between HPC and Cloud here in this paragraph a lot better}
While Cloud computing is changing IT infrastructure, it also has had a drastic impact on Distributed Systems itself, which has a different evolution. Gone are the IBM Mainframes of the 1980's which dominated the enterprise landscape.  While some mainframes still exist, they are used only for batch related processing tasks and are relatively unused for scientific applications as they are inefficient at Floating Point Operations.  As such, they were replaced with Beowulf Clusters \cite{sterling2001beowulf} of the 90's and 00's. A novelty of Supercomputing is that instead of just one large machine, many machines are connected together and used to achieve a common goal, thereby maximizing the overall speed of computation.  Clusters represent a more commodity-based supercomputer, where off the shelf CPUs are used instead of the highly customized and expensive processors in Supercomputers.  

Supercomputers and Clusters are best suited for large scale applications.  These HPC applications can even include ``Grand Challenge" applications \cite{hoare2005grand} and can represent a sizable amount of the scientific calculations done on large-scale Supercomputing resources today. However, there exists a gap of many orders of magnitude  between leading-class high performance computing and what is available on the common laboratory workshop. This gap, described here as mid-tier scientific computation is a fast growing field that struggles to efficiently harness distributed systems while hoping to minimize extensive development efforts. These mid-tier scientific endeavours need to leverage distributed systems to effectively complete the calculations at hand, however may not require the extreme scale provided by the latest machines at the peak of the Top500 list \cite{www-top500}. Furthermore, it may not be feasible for the research teams to properly handle the development complexity of such resources.  This can include scientific disciplines such as high energy physics \cite{buncic2010cernvm}, materials science \cite{wang2006survey}, bioinformatics \cite{menon2012cloud}, and climate research \cite{He2010nasa}, to name a few.  


As more domain science turns to the aid of computational resources for conducting novel scientific endeavours, there is a continuing and growing need for national cyberinfrastructure initiatives to support an increasingly diverse set of scientific workloads. Substantial growth can be see in the number of computational resource requests \cite{towns2014xsede, antypas2008nersc} from many of the larger computational facilities.  Concurrently, there has also been an increase in accelerators and hybrid computing models capable of quickly providing additional resources \cite{vetter2011keeneland} beyond commodity clusters.

Historically, application diversity was separated into High Performance Computing (HPC) and High Throughput Computing (HTC).  With HTC, computational problems can be separated into independent tasks that can execute in a pleasingly parallel fashion, happily gaining any available resources and rarely require significant communication or synchronization between tasks. HPC often represents computational problems that require significant communication and coordination to effectively produce results, usually with the use of a communication protocol such as MPI \cite{mpi}.  Recently however, many big-data paradigms \cite{agrawal2011big} have been introduced in distributed systems that represent new computational models for distributed computing, such as MapReduce \cite{dean2008mapreduce} and the corresponding Apache Big Data stack \cite{Jha2014apache}. Supporting these different distributed computational paradigms requires a flexible infrastructure capable of providing computational resources for all possible models in a fast and efficient manner. 

\TODO{We need something meta here.  A bit historical, bring in cloud computing}

\TODO{Introduce virtual clusters here, related to beowulf clusters, hpc systems}

One such proposed solution is the use of virtual clusters \cite{Foster2006} to provide distinct, separated environments on similar resources using virtual machines. These virtual clusters, similar in design to the Beowulf clusters and commodity HPC systems, provide a distributed memory environment, usually with a local resource management system \cite{czajkowski1998resource}, however they do so in Virtual Machines (VMs).  However, past concerns with virtualization have limited the adoption of virtual clusters in many large scale cyberinfrastructure deployments. This has largely been due to the overhead of virtualization, whereby many scientific computations have experienced a significant and notable degradation in performance.  In an ecosystem familiarized with HPC systems where performance is paramount, this has been an obstructive hurdle for many even mid-level scientific endeavors.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Statement}
\label{sec:stmt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\TODO{The open question is if Cloud infrastructure, using virtualizaton, can support mid-tier scientific computation.  This argument is that yes, it potentially can.}

With the rise of cloud computing within the greater realm of distributed systems, we've seen a number of scientific computing endeavors map well to cloud infrastructure. First, this includes the simple and most common practice of on demand computing where by users can rent-a-workstation \cite{kondo2009cost}. Perhaps these resources are more powerful than a given researcher's laptop and used to run their scientific applications or support greater laboratory collaborative efforts, such as a shared database or web service. 

Second, we've seen virtualized cloud infrastructure support high throughput computing very well. Often times pleasingly parallel applications, be it from high energy physics such as the LHC effort \cite{buncic2010cernvm, bell2015scaling} or bioinformatics with BLAST alignment jobs \cite{menon2012cloud}, have proven to run with high efficiency in public and private cloud environments. The rise of public cloud infrastructure has also coincided with increase in big data computation and analytics.
\TODO{give more info on Big data here}.
Many of these big data platform services have evolved complimentary to cloud infrastructure, and as such have a symbiotic relationship with virtualization technologies \cite{gunarathne2010mapreduce}.  

However, often times in the past tightly coupled, high performance distributed memory applications, often the same endeavors that support leading class scientific efforts, run very poorly on virtualized cloud infrastructure \cite{ostermann2009performance}.  This is due to a myriad of addressable reasons ranging from scheduling, abstraction overhead, or a lack of advanced hardware support necessary for tightly coupled communication. This postulates the question on weather virtualization can in fact support such tightly coupled large scale applications without an imposed significant performance penalty. The goal of this dissertation is to investigate the viability of mid-tier scientific applications supported in a virtualized infrastructure.   

%\TODO{Mention virtual clusters quickly here}

%\TODO{Discuss cloud vs hpc}

Given the current outlook on virtualization for supporting HPC applications, this dissertation proposes a framework for High Performance Virtual Clusters that enable advanced computational workloads, including tightly coupled distributed memory applications, to run with a high degree of efficiency with virtualization technologies. This framework, outlined in Figure \ref{F:framework}, illustrates the topics to be addressed to provide a supportive virtual cluster environment for high performance mid-tier scientific applications.  We specifically identify mid-tier distributed memory computations as a focal point for the computational challanges at hand as a way to separate from some of the latest efforts in towards Exascale \cite{dongarra2011exascale, bergman2008exascale, shalf2010exascale} computing.  While virtualization may in fact be able to play a role towards usable Exascale computing, such efforts fall outside the immediate scope of this dissertation. 

 \FIGURE{htb}
  {images/VirtualClusterFramework.pdf}
  {1.0}
  {High Performance Virtual Cluster Framework}
  {F:framework}

In order to provide comprehensive high performance virtual clusters, we need to first consider the most important aspect, the virtualized infrastructure itself. At the core, we have to consider the hypervisor or virtual machine monitor and the overhead and performance characteristics associated with it. This includes performance tuning considerations, NUMA effects, and advanced hardware device passthrough. Specifically, device passthrough in the context of this manuscript refers to two major concentrations: GPUs and InfiniBand interconnects (the later using SR-IOV). The virtual infrastructure also must consider scheduling as a major factor in performing efficient placement of workloads on the underlying host infrastructure, and in particular a Proximity scheduler is of interest. Storage solutions in a virtualized environment is an increasingly important aspect of this framework, as both HPC and big data solutions are continuing to prioritize I/O performance compared to computation. Storage is also likely to be heavily dependent on interconnect considerations as well, as potentially provided by device passthrough.  

However, providing an enhanced virtualized infrastructure does not solve all aspects of high performance virtual clusters. Specifically, proposed infrastructures need to be properly evaluated in a systematic way through the use of a wide array of benchmarks, mini-applications, and full-scale scientific applications. This effort can further be broken separated into three major problem sets; base level benchmarking tools, HPC applications, and big data applications. Evaluating the stringent performance requirements of all three sets, when compared with bare metal (no virtualization) common solutions, will illuminate not only successful designs but also the necessary focus areas that require more attention.  As such, we look to continually use these benchmarks and applications as a tool to measure the viability of virtualization in this context. 

\TODO{Describe the vision with OpenStack, and bring in the architecture diagram}
From the framework and the research found... blah ... an architecture is found.  

 \FIGURE{htb}
  {images/VirtualCluster-architecture.pdf}
  {1.0}
  {Architectural diagram for High Performance Virtual Clusters}
  {F:hpvcarch}


One of the higher-level aspects of providing high performance virtual clusters is the high level orchestration of the virtual clusters themselves. While this largely remains tangential to this research, it is none the less a key aspect for a successful solution. Some effort has been put forth for virtual cluster experiment management \cite{las2010gce}, and many ongoing open sources solutions also offer compelling options, such as OpenStack Heat \cite{www-openstack-heat}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Challenges}
\label{sec:chall}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The framework, architecture, and efforts described in this dissertation represent a movement forward in providing virtualized infrastructure to support a wide arrange of scientific applications. However, there still exist some challenges that will need to be addressed.  This includes a sigma of virtualization being inherently slow and unable to support tightly coupled computations, limitations with advancing at scale, and even that containers may provide a better alternative.  While this work hopes to move beyond these challenges, they none the less must be considered. 

The notion that virtualization and Cloud infrastructure are not able to support parallel distributed memory applications has been characterized many times. One of the most prominent examples of this is the Department of Energy's Magellan Project \cite{www-magellan}, where by the Magellan Final Report \cite{MagellanFinal} states the following finding as a Key Finding:
  
\begin{quote}
``\textbf{Finding 2. Scientific applications with minimal communication and I/O are best suited for
clouds.}

We have used a range of application benchmarks and micro-benchmarks to understand the performance of scientific applications. Performance of tightly coupled applications running on virtualized clouds using commodity networks can be significantly lower than on clusters optimized for these workloads. This can be true at even mid-range computing scales. For example, we observed slowdowns of about 50x for PARATEC on the Amazon EC2 instances compared to Magellan bare-metal (non-virtualized) at 64 cores and about 7x slower at 1024 cores on Amazon Cluster Compute instances, the specialized high performance computing offering. As a result, current cloud systems are best suited for high-throughput, loosely coupled applications with modest data requirements.``
\end{quote}

These findings underscore how classical usage of virtualization in cloud infrastructure has serious performance issues when running tightly coupled distributed memory applications. Many of these performance concerns are sound, given the limitation of a number of virtualization overheads commonplace at the time, including shadow page tables, slow emulated Ethernet drivers, experimental hypervisors, and a complete lack of sophisticated hardware concurrently was commonplace in supercomputers and clusters.  As a result, the advantages of virtualization, including on-demand resource allocation, live migration and advanced hybrid migration, and user-defined environments, have not been able to effectively show their value in the context of the HPC community.

Other and related efforts within the scientific community too found limiations with HPC applications in public cloud environments. This includes the study by Jackson et. al \cite{jackson2010performance} which illustrates how Amazon Ec2 creates a 6x performance impact compared to a local cluster, due to a large part on the limiting Gigabit Ethernet that benchmarks relied heavily on within the EC2 system. Other studies also found similar results such as Ostermann \cite{ostermann2009performance}, condluding that Amazon EC2 ``is insufficient for scientific computing at large, though it still appeals to the scientists that need resources immediately and temporarily."  However, these studies are now outdated and do not take into account the advancements in virtualization and hardware availability detailed in this dissertation. Specifically, it is estimated that with the KVM hypervisor in a performance-tuned environment, using accellerators and most certainly a high-speed, low latency interconnect as detailed in Chapter \ref{chap:mdsimulations}, the results would be drastically different. 


Another limitation in high performance virtual clusters is the degree to which applications can scale in such environments remains relatively unknown. While initial results with SR-IOV InfiniBand are promising, scaling is naturally hard to predict. While unfounded, it would be hypothetically possible that as the number of VM's increases, tail-latency could also increase, causing notable slowdowns during distributed memory synchronization barriers. It is only when infrastructure comes available to support high performance virtual clusters will scaling beyond to thousands of cores and beyond be feasible.  


\TODO{Mention competition with containerization}

\TODO{HPC/Big data convergence. Need to provide functionality for both}



- However, historically virtualization has performance issues
- Performance must be a first class function
- Need for a framework.  to organize efforts
- Need to address not only HPC but also big data
- HPC and Big data convergence 



However, virtualization itself may not be fundamentally limited by the overhead that causes issues in running high performance computing applications. Recent improvements in performance, along with increased usability of accelerators and high speed, low latency interconnects in virtualized environments as demonstrated in this dissertation, have made virtual clusters a more viable solution for mid-tier scientific applications.  Furthermore, it is possible for  virtualization technologies to bring enhanced usability and enable specialized runtime components to future HPC resources, adding significant value over today's supercomputing resources.  This could potentially include infrastructure advances for higher level cloud platform services for supporting big data applications \cite{qiu2014towards}. 




%\subsection{Contributions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}
\label{sec:outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The rest of this dissertation is organized into chapters, each signifying the steps to move forward the notion of a high performance virtual cluster solution.

Chapter \ref{chap:related} investigates the related research surrounding both cloud computing and high performance computing. Within cloud computing, an introduction to cloud infrastructure, virtualization, and containers will all be discussed. This also includes details regarding virtual clusters as well as an overview of some national scale cloud infrastructure efforts that exist. Furthermore, we investigate the state of high performance computing and supercomputing, as well touch upon some of the current Exascale efforts.

Chapter \ref{chap:cloud2011} takes a look at the potential for virtualization, in its current state, for high performance computing. This includes a feature comparison for hardware availability of a few common hypervisors, specifically Xen, KVM, VirtualBox, and VMWare. Then, a few common HPC benchmarks are evaluated to determine what overhead exists and where in a single node configuration. This identifies how in some scenarios, virtualization adds only a minor overhead, whereas with other scenarios, overheads can be up to 50\% compared to native configurations. 

Chapter \ref{chap:hpgc2014} starts to overcome one of the main limitations of virtualization for use in advanced scientific computing, specifically the lack of hardware availability. In this chapter, The Xen hypervisor is used to demonstrate the first publicized affect of GPU Passthrough, allowing for GPUs to be used in a guest VM. The efficiency of this method is briefly evaluated using two different hardware setups, and finds hardware can play a notable role in single node performance. 

Chapter \ref{chap:cloud2014} continues where chapter \ref{chap:hpgc2014} leaves off, by demonstrating that GPU passthrough is possible on many other hypervisors, specifically also KVM and VMWare, and compares with one of the main containerization solutions, LXC. Here, the GPUs are evaluated using not only the HPC SHOC GPU benchmark suite developed at Oak Ridge National Laboratory, but also a diverse mix of real-world applications to examine how and where overhead exists with GPUs in VMs for each virtualization setup.  Specifically, we find that with properly tuned hardware and NUMA-balanced configurations, that the KVM solution can perform at roughly near-native performance, with on average ~1.5\% overhead compared to no virtualization. This illustrates that with the combination correct hypervisor selection, careful tuning, and advanced hardware, scientific computations can be supported using virtualized hardware. 

Chapter \ref{chap:mdsimulations} takes the findings from the previous chapter to the next level. Specifically, the lessons learned from successful KVM virtualization with GPUs is expanded and combined with a missing key component of supporting advanced parallel computations: a high speed, low latency interconnect, specifically InfiniBand. Using SR-IOV and PCI Passthrough of QDR InfiniBand interconnect across a small cluster, it is demonstrated that two Molecular Dynamics simulations, both very commonly used in the HPC community, can be run at near-native performance in the designed virtual cluster.

Chapter \ref{chap:future-work} takes a look at the given situation, and puts forth an argument for a high performance virtual cluster solutions. Specifically, we look at the given state of the art, how virtual clusters can be used to provide an infrastructure to support the convergence between HPC and big data. Furthermore, this chapter outlines and investigates potential next steps for virtualization, including the potential for advanced live migration techniques and VM cloning, which can be made available with the inclusion of a high-performance RDMA-capable interconnect. 

Finally, this work concludes with an overall view of the current state of high performance virtualization, as well as it's potential to impact and support a wide array of disciplines. 



