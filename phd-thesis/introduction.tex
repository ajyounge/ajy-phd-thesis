%% Author: Andrew J. Younge
%% PhD Thesis

%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%
\chapter{Introduction}
\label{chap:intro}
%$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Overview}
\label{sec:overview}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{comment}
\TODO{Give quick very high level view. 

Provide insightful quote regarding scientific computation 

Breifly describe current state of Distributed Systems.

Introduce the importance and impact of HPC.

Introduce need/desire for flexible utility computing.
- list major virtualization advantages

Describe major differences between HPC and cloud

Define how a merger between HPC and Cloud computing is desired.

Introduce how Virtual Clusters, can do this.
- what is a virtual cluster? What is it not?
- Missing performance considerations.

}
\end{comment}


For years, visionaries in computer science have predicted the advent of utility-based computing.  This concept dates back to John McCarthy's vision stated at the MIT centennial celebrations in 1961:

\begin{quote}
``If computers of the kind I have advocated become the computers of the future, then computing may someday be organized as a public utility just as the telephone system is a public utility... The computer utility could become  the basis of a new and important industry.``
\end{quote}
 Only recently has the hardware and software become available to support the concept of utility computing on a large scale.

The concepts inspired by the notion of utility computing have combined with the requirements and standards of Web 2.0 \cite{alexander2006wnw} to create Cloud computing \cite{buyya2008moc, foster2008cca, aboveTheClouds}.  Cloud computing is defined as ``A large-scale distributed computing paradigm that is driven by economies of scale, in which a pool of abstracted, virtualized, dynamically-scalable, managed computing power, storage, platforms, and services are delivered on demand to external customers over the Internet." This concept of cloud computing is important to Distributed Systems because it represents a true paradigm shift \cite{kuhn1970structure} within the entire IT infrastructure.  Instead of adopting the in-house services, client-server model, and mainframes, clouds push resources out into abstracted services hosted \textit{en masse} by larger organizations.  This concept of distributing resources is similar to many of the visions of the Internet itself, which is where the ``clouds'' nomenclature originated, as many people depicted the internet as a big fluffy cloud one connects to.

At the core of most cloud infrastructure lies virtualization, a computer architecture technology by which 1 or more Virtual Machines (VMs) are run on the same physical host. In doing this, a layer of abstraction is inserted between and around the hardware and Operating System (OS). Specifically, hardware resources such as CPUs, memory, and I/O devices, and software resources analagous to OS functionality and low level libraries are abstracted and provided to VMs directly. Virtualization has existed for many years, but its availability with Intel x86 commodity hardware in conjunction with the rise of clouds has brought it to the forefront of distributed systems. 

%Should this go into related research?
%\TODO{Blend between HPC and Cloud here in this paragraph a lot better}
While cloud computing is changing IT infrastructure, it also has had a drastic impact on distributed systems as a field,  which has a different evolution. Gone are the IBM Mainframes of the 1980s, which dominated the enterprise landscape.  While some mainframes still exist, today they are used only for batch related processing tasks and not for scientific applications as they are inefficient at Floating Point Operations.  As such, Beowulf Clusters \cite{sterling2001beowulf}, Massively Parallel Processors (MPPs) and Supercomputers of the 90s and 00s replaced the mainframes of before. A novelty of these distributed memory systems is that instead of just one large machine, many machines are connected together to achieve a common goal, thereby maximizing the overall speed of computation.  Clusters represent a more commodity-based supercomputer with off-the-shelf CPUs instead of the highly customized and expensive processors and interconnects found in Supercomputers.  
Supercomputers and Clusters are best suited for large-scale applications, due to their innate ability to divide the computational efforts into smaller concurrent tasks. These tasks often run in parallel on many CPUs, whereby each task communicates results to other tasks in some way as per the application design. These HPC applications can even include ``Grand Challenge" applications \cite{hoare2005grand} and can represent a sizable amount of the scientific calculations done on large-scale Supercomputing resources today. However, there exists a gap of many orders of magnitude  between leading-class high performance computing and what is available on the common laboratory workshop. This gap, described here as mid-tier scientific computation, is a fast growing field that struggles to harness distributed systems efficiently while hoping to minimize extensive development efforts. These mid-tier scientific endeavors need to leverage distributed systems to complete the calculations at hand effectively, however, they may not require the extreme scale provided by the latest machines at the peak of the Top500 list \cite{www-top500}.  Some small research teams may not have the resources available or the desire to handle the development complexity of high end supercomputing systems, and can look towards other options instead.  This can include some scientific disciplines such as high energy physics \cite{buncic2010cernvm}, materials science \cite{wang2006survey}, bioinformatics \cite{menon2012cloud}, and climate research \cite{He2010nasa}, to name a few.  


As more domain science turns to the aid of computational resources for conducting novel scientific endeavours, there is a continuing and growing need for national cyberinfrastructure initiatives to support an increasingly diverse set of scientific workloads. Historically, there have been a number of national and international cyberinfrastructure efforts to support high end computing. These range from traditional supercomputers deployed at Department of Energy computing facilities \cite{bland2012titan}, to efforts led by the National Science Foundation such as the XSEDE environment \cite{towns2014xsede} or domain specific initiatives such as the NSF iPlant collaborative \cite{goff2011iplant}.Substantial growth can be seen in the number of computational resource requests \cite{towns2014xsede, antypas2008nersc} from many of the larger computational facilities.  Concurrently, there has also been an increase in accelerators and hybrid computing models capable of quickly providing additional resources \cite{vetter2011keeneland} beyond commodity clusters.
 
Historically, application diversity was separated into High Performance Computing (HPC) and High Throughput Computing (HTC).  With HTC, computational problems can be split into independent tasks that execute in a pleasingly parallel fashion, happily gaining any available resources and rarely requiring significant communication or synchronization between tasks.  Example of this can include independent tasks or parameter sweeps of an application to find the optimal application settings, or many similar tasks with only slightly different input datasets.  HPC often represents computational problems that require significant communication and coordination to produce results effectively, usually with the use of a communication protocol such as MPI \cite{mpi}. These applications are often tightly coupled sequential tasks operating concurrently and communicating often to complete the collective computational problem.  While there are many examples of such HPC applications, common examples often involve significant matrix and/or vector multiplication mechanisms \cite{fox1987matrix} for completing simulations. 

However, many big-data paradigms \cite{agrawal2011big} in distributed systems have been introduced that represent new computational models for distributed computing, such as MapReduce \cite{dean2008mapreduce} and the corresponding Apache Big Data stack \cite{kamburugamuve2013survey, chen2014big}.  While the term big data can mean a number of things, it generally refers to data sets large or complex enough to require advanced, non-traditional data processing mechanisms in order to extract feature knowledge. Some HTC and HPC applications could potentially be considered big data tools, generally the nomenclature has grown out of new industry and academic problem sets created by a data deluge \cite{bell2009deluge}.  This has lead to a number of novel platform services for handling big data problems in parallel, and supporting these different distributed computational paradigms requires a flexible infrastructure capable of providing computational resources for all possible models in a fast and efficient manner.  


Currently, we are at the forefront of a convergence within scientific computing between HPC and big data computation \cite{reed2015exascale}. This amalgamation of historically differing viewpoints of Distributed Systems looks to combine the performance characteristics of HPC and the pursuit towards Exascale with the data and programmer oriented concurrency models found in Big Data and cloud services. 

Much of the convergence effort has been focused on applications and platform services. Specifically, significant work towards convergent applications has been outlined with the Big Data Ogres \cite{Jha2014apache} and the corresponding HPC-ABDS model \cite{qiu2014towards}.  This convergence can also be seen with efforts in bringing interconnect advances to classically big data platforms such as with InfiniBand and MapReduce \cite{panda2013hadoop}. However, the underlying hardware and OS environments are still something to be reconciled, which is something that virtualization can potentially help provide. It is expected that new big data efforts will continue to move in this direction \cite{ekanayake2016spidal}, especially if virtualization can make HPC hardware that's traditionally prohibitive in such areas, such as accelerators and high-speed interconnects, readily available to cloud and big data platforms. As the deployment of big data applications and platform services on virtualized infrastructure is well defined and studied \cite{tian2011towards}, this work instead focuses on the difficulty of running HPC applications on similar virtualized infrastructure.  However, it is possible and hopeful that research regarding virtualization can also play a part in bringing advanced hardware and performance-focused considerations to Big Data applications, effectively cross-cutting the convergence with HPC. In Summary, success of the research in virtualization could be defined by the ability to support the convergence between HPC and Big Data.

\FIGURE{htb}
 {images/convergent-ecosystem.pdf}
 {1.0}
 {Data analytics and computing ecosystem compared (from \cite{reed2015exascale}), with virtualization included}
 {F:convergence}


To further illustrate where virtualization can play a part in HPC and Big Data convergence, we look at Figure \ref{F:convergence} from Reed \& Dongarra \cite{reed2015exascale}. While the two ecosystems depicted are only representative and in no way exhaustive, they do show how drastically different user environments are and how reliant they are on differing hardware. If we insert a performance-oriented virtualization mechanism within the system software capable of handling the advanced cluster hardware performing at near-native speeds (at or under 5\% overhead, as loosely defined in \cite{lange2010palacios}), it could provide a single, comprehensive \emph{convergent ecosystem} that supports both HPC and Big Data efforts at a critical level. 


This work proposes the use of virtual clusters \cite{Foster2006} to provide distinct, separate environments on similar resources using virtual machines. These virtual clusters, similar in design to the Beowulf clusters and commodity HPC systems, provide a distributed memory environment, usually with a local resource management system \cite{czajkowski1998resource}.  However, past concerns with virtualization have limited the adoption of virtual clusters in many large-scale cyberinfrastructure deployments. This has in part been due to the overhead of virtualization, whereby many scientific computations have experienced a significant and notable degradation in performance.  In an ecosystem familiar with HPC systems in which performance is paramount, this has been an obstructive hurdle for deploying many tightly coupled applications. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Statement}
\label{sec:stmt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\TODO{The open question is if Cloud infrastructure, using virtualizaton, can support mid-tier scientific computation.  This argument is that yes, it potentially can.}

With the rise of cloud computing within the greater realm of distributed systems, there have been a number of scientific computing endeavors that map well to cloud infrastructure. This first includes the simple and most common practice of on-demand computing whereby users can rent-a-workstation \cite{kondo2009cost}. Perhaps these resources are more powerful than a given researcher's laptop and used to run their scientific applications or support greater laboratory collaborative efforts, such as a shared database or Web services.  We have also seen virtualized cloud infrastructure support high throughput computing very well. Often times pleasingly parallel applications, be it from high energy physics such as the LHC effort \cite{buncic2010cernvm, bell2015scaling} or bioinformatics with BLAST alignment jobs \cite{menon2012cloud}, have proven to run with high efficiency in public and private cloud environments. Furthermore, the rise of public cloud infrastructure has also coincided with increase in big data computation and analytics.  Many of these big data platform services have evolved complimentary to cloud infrastructure, and as such have a symbiotic relationship with virtualization technologies \cite{gunarathne2010mapreduce}.  

However, with tightly coupled, high performance distributed memory applications, the same endeavors that support leading class scientific efforts, run very poorly on virtualized cloud infrastructure \cite{ostermann2009performance}.  This is due to a myriad of addressable reasons ranging from scheduling, abstraction overhead, and a lack of advanced hardware support necessary for tightly coupled communication. This postulates the question regarding whether virtualization can in fact support such tightly coupled large-scale applications without an imposed significant performance penalty. Simply put, \emph{the goal of this dissertation is to investigate the viability of mid-tier scientific applications supported in virtualized infrastructure}.  

Historically, mid-tier scientific applications are distributed memory HPC applications that require more complex process communication mechanisms. These systems need far more performance than a single compute resource (such as a workstation) can provide. This could include hundreds or thousands of processes calculating and communicating concurrently on a cluster, perhaps using a messaging interface such as MPI.  These applications can be distinct (and sometimes simpler), either in application composition or operating parameters, from extreme-scale HPC applications that run at the highest end of the supercomputing resources today operating on petascale machines and beyond.  


%\TODO{Mention virtual clusters quickly here}

%\TODO{Discuss cloud vs hpc}

Given the current outlook on virtualization for supporting HPC applications, this dissertation proposes a framework for High Performance Virtual Clusters that enable advanced computational workloads, including tightly coupled distributed memory applications, to run with a high degree of efficiency  in virtualized environments. This framework, outlined in Figure \ref{F:framework}, illustrates the topics to be addressed to provide a supportive virtual cluster environment for high performance mid-tier scientific applications.  Areas marked in darker green indicate topics this dissertation may touch upon, whereas light green areas in Figure \ref{F:framework} identify outstanding considerations to be investigated. Specifically, mid-tier distributed memory parallel computations is identified as a focal point for the computational challenges at hand as a way to separate from some of the latest efforts towards Exascale computing \cite{dongarra2011exascale, bergman2008exascale, shalf2010exascale}.  While virtualization may in fact be able to play a role towards usable Exascale computing, such efforts fall outside the immediate scope of this dissertation. 


 \FIGURE{htb}
  {images/VirtualClusterFramework.pdf}
  {1.0}
  {High Performance Virtual Cluster Framework}
  {F:framework}

In order to provide high performance virtual clusters, there is a need to first look at a key area, the virtualized infrastructure itself. At the core, the hypervisor, or virtual machine monitor, has to be considered and the overhead and performance characteristics associated with it. This includes performance tuning considerations, non-uniform memory access (NUMA) effects, and advanced hardware device passthrough. Specifically, device passthrough in the context of this manuscript refers to two major device types; GPUs and InfiniBand interconnects (the later using SR-IOV). The virtual infrastructure also must consider scheduling as a major factor in performing efficient placement of workloads on the underlying host infrastructure, and in particular a Proximity scheduler is of interest \cite{www-proximity-scheduler}. Storage solutions in a virtualized environment is an increasingly important aspect of this framework, as some HPC and big data solutions are prioritizing I/O performance more than computation. Storage is also likely to be heavily dependent on interconnect considerations as well, which could be potentially provided by device passthrough. However, such I/O considerations lie beyond this dissertation's immediate scope.  


However, simply providing an enhanced virtualized infrastructure may not guarantee that all implementations of high performance virtual clusters are performant. Specifically, proposed infrastructures need to be properly evaluated in a systematic way through the use of a wide array of benchmarks, mini-applications, and full-scale scientific applications. This effort can further be separated into three major problem sets; base level benchmarking tools, HPC applications, and big data applications. Evaluating the stringent performance requirements of all three sets, when compared with bare metal (no virtualization) solutions, will illuminate not only successful designs but also the focus areas that require more attention.  As such, we look to continually use these benchmarks and applications as a tool to measure the viability of virtualization in this context. 

Fundamentally, this framework is leveraging these real-world applications and benchmarks predominantly for the evaluation of various solutions under strong scaling conditions. Each evaluating metric will generally apply a fixed problem size (or predefined set of fixed problem sizes) that look to match real-world running conditions, and vary the number of processors and execution units (including GPUs) across multiple virtualized and non-virtualized architectures.  Generally, a successful solution will demonstrate as close to a native, bare-metal runtime as possible, illustrating little overhead of the added methods. Furthermore, variability between runtimes are also important to consider, as high variability and noise within large scale distributed systems can lead to tail latencies and significant efficiency losses.

 \FIGURE{htb}
  {images/VirtualCluster-architecture.pdf}
  {1.0}
  {Architectural diagram for High Performance Virtual Clusters}
  {F:hpvcarch}

Starting from the historical perspective of virtual clusters in Grid computing, we see the new architectural model for high performance virtual clusters illustrated in Figure \ref{F:hpvcarch} that is implied by the efforts described in this dissertation.  Here, we leverage commodity hardware, as well as some advanced HPC hardware withint he same system. Providing advanced hardware in this scenario has a twofold effect. First, we look to provide classic distributed memory HPC applications in an virtualized environment that still utilize the same hardware that such applications have grown accustomed to.  Second, it allows for novel big data analytics and cloud platform services to move towards hardware that may drastically speed up their computational efforts. Specifically, we allow for such platform services to leverage the high-speed, low-latency interconnects that HPC systems have relied upon. While such cloud systems may not be able to take full advantage of the latency improvements, it is estimated the the increased bandwidth may have a significant impact on the overall runtime of these systems, reducing the overall time-to-solution. While this notion could incorporate a wide array of differing technologies, we focus here on GPU-based accelerators and InfiniBand interconnects in conjunction with x86 CPUs. 

From this diverse yet highly optimized hardware, we leverage KVM and QEMU to provide an advanced hypervisor for creating and hosting VMs with direct hardware involvement from the lower level.  Throughout this dissertation, the utilization of this diverse hardware can be wrangled and virtualized through the use of KVM and QEMU. This includes direct virtualization, para-virtualization with QEMU, and even direct passthrough mechanisms.  Moving up, the Libvirt API is leveraged due to its hypervisor interoperability and popularity. Modifications are made throughout this work to enable Libvirt to support passthrough mechanisms in both Xen and KVM, changes which since have since made their way to libvirt upstream.  Providing Libvirt as a virtualization API also insulates from higher level changes, whereby other resource management solutions could be swapped in if such a design was warranted. 

Building form Libvirt comes the OpenStack private cloud infrastructure. In Figure \ref{F:hpvcarch} we illustrate some (but not all) of OpenStack's services including the Horizon UI, Cinder and Glance storage mechanisms, and the Neutron (previously Quantum) components. The Nova component of OpenStack is the point of focus for providing comprehensive VM management.
\footnote{While many of the features for nova's additions in GPU Passthrough and SR-IOV InfiniBand support have been put together at USC/ISI as an OpenStack Nova fork (https://libraries.io/github/usc-isi/nova), the features have since been modified and matured by the OpenStack community in later releases and made available in upstream Nova.}  OpenStack provides a comprehensive cloud infrastructure service for managing up to thousands of nodes in a cluster using virtualization. Its integration with KVM through Libvirt is highly tuned for efficient  VM deployment and advanced plug-ins such as the ML2 Neutron service can managed an advanced interconnect such as InfniBand and 40GbE provided by Mellanox. 

Atop OpenStack, we can create a wide array of virtual clusters and machines to support diverse scientific computing ecosystems necessary. This includes application models ranging from tightly coupled MPI+CUDA HPC applications, to emerging big data analytics toolkits such as Apache Storm \cite{kamburugamuve2016streaming}. Such virtual clusters can be built manually through users requesting highly tuned images from a repository and deploying them to scale. This also allows for the use of virtualized SMP systems such as ScaleMP \cite{Younge2016smp}.

One of the higher-level aspects of providing high performance virtual clusters is the orchestration of the virtual clusters themselves, which could be called experiment management. While this largely remains tangential to this immediate research, it is nonetheless a key aspect for a successful solution. Some effort has been put forth for virtual cluster experiment management \cite{las2010gce}, and many ongoing open sources solutions also offer compelling options, such as OpenStack Heat \cite{www-openstack-heat}.  An example of a project delivering advanced orchestration mechanisms and a toolkit to aid in configurable virtual clusters on heterogeneous IaaS deployments is the Cloudmesh effort \cite{von2014cloudmesh}.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Research Challenges}
\label{sec:chall}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The framework, architecture, and efforts described in this dissertation represent a movement forward in providing virtualized infrastructure to support a wide arrange of scientific applications. However, there still exist some challenges that will need to be addressed.  This includes a stigma of virtualization being inherently slow and unable to support tightly coupled computations, limitations associated with  operating at scale, and even that containers may provide a better alternative.  While this work hopes to move beyond these challenges, they none the less must be considered. 

The notion that virtualization and Cloud infrastructure are not able to support parallel distributed memory applications has been characterized many times. One of the most prominent examples of this is the Department of Energy's Magellan Project \cite{www-magellan}, whereby the Magellan Final Report \cite{MagellanFinal} states the following finding as a Key Finding:
  
\begin{quote}
``\textbf{Finding 2. Scientific applications with minimal communication and I/O are best suited for clouds.}

We have used a range of application benchmarks and micro-benchmarks to understand the performance of scientific applications. Performance of tightly coupled applications running on virtualized clouds using commodity networks can be significantly lower than on clusters optimized for these workloads. This can be true at even mid-range computing scales. For example, we observed slowdowns of about 50x for PARATEC on the Amazon EC2 instances compared to Magellan bare-metal (non-virtualized) at 64 cores and about 7x slower at 1024 cores on Amazon Cluster Compute instances, the specialized high performance computing offering. As a result, current cloud systems are best suited for high-throughput, loosely coupled applications with modest data requirements.``
\end{quote}

These findings underscore how classical usage of virtualization in cloud infrastructure has serious performance issues when running tightly coupled distributed memory applications. Many of these performance concerns are sound, given the limitation of a number of virtualization overheads commonplace at the time, including shadow page tables, emulated Ethernet drivers, experimental hypervisors, and a complete lack of sophisticated hardware commonplace in supercomputers and clusters.  As a result, the advantages of virtualization, including on-demand resource allocation, live migration and advanced hybrid migration, and user-defined environments, have not been able to show effectively their value in the context of the HPC community.

Other and related efforts within the scientific community too found limitations with HPC applications in public cloud environments. This includes the study by Jackson et. al \cite{jackson2010performance} which illustrates how the Amazon Elastic Compute Cloud (EC2) creates a 6x performance impact compared to a local cluster, due in large part to the limiting Gigabit Ethernet on which benchmarks relied heavily within the EC2 system. Other studies also found similar results; Ostermann \cite{ostermann2009performance} for instance, concludes that Amazon EC2 ``is insufficient for scientific computing at large, though it still appeals to the scientists that need resources immediately and temporarily."  However, these studies are now outdated and do not take into account the hardware and advancements in virtualization detailed in this dissertation. Specifically, it is estimated that with the KVM hypervisor in a performance-tuned environment, using accelerators and most certainly a high-speed, low latency interconnect as detailed in Chapter \ref{chap:mdsimulations}, the results would be drastically different. 


One limitation in this research in high performance virtual clusters is the fact that the degree to which applications can scale remains relatively unknown. While initial results with SR-IOV InfiniBand are promising, scaling is naturally hard to predict. It would be hypothetically possible that as the number of VM's increases, interconnect communication tail-latency could also increase, causing notable slowdowns during distributed memory synchronization barriers. It is only when infrastructure able to support high performance virtual clusters becomes available will scaling to thousands of cores and beyond be investigated.  

Another potential challenge, and perhaps also a strength, is the rising use of containers within industry, such as we see in efforts like Docker \cite{merkel2014docker}. Recently, we have seen efforts at NERSC/LBNL adapt a container solution called Shifter with the SLURM resource manager on CRAY XC based systems \cite{jacobsen2015contain}. Shifter's goal is to provide user defined images for NERSC's bioinformatics users, and it adapts remarkably well to the HPC environment. While further efforts are needed by Cray and NERSC to fully provide a container-based solution on a large-scale Supercomputer for all applications, its efforts are in many ways related to virtualization. In Chapter \ref{chap:cloud2014}, we specifically compare virtualization efforts with LXC \cite{xavier2013performance}, a popular Linux container solution, and find performance to be comparable and largely near-native. 

 
While the major concern with virtualization in the HPC community is performance issues, virtualization itself may not be fundamentally limited by the overhead that causes issues in running high performance computing applications. Recent improvements in performance, along with increased usability of accelerators and high speed, low latency interconnects in virtualized environments, as demonstrated in this dissertation, have made virtual clusters a more viable solution for mid-tier scientific applications.  Furthermore, it is possible for  virtualization technologies to bring enhanced usability and enable specialized runtime components to future HPC resources, adding significant value over today's supercomputing resources.  This could potentially include infrastructure advances for higher level cloud platform services for supporting big data applications \cite{qiu2014towards}. 




%\subsection{Contributions}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Outline}
\label{sec:outline}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


The rest of this dissertation is organized into chapters, each signifying the steps to move forward the notion of a high performance virtual cluster solution.

Chapter \ref{chap:related} investigates the related research surrounding both cloud computing and high performance computing. Within cloud computing, an introduction to cloud infrastructure, virtualization, and containers will all be discussed. This also includes details regarding virtual clusters as well as an overview of some national scale cloud infrastructure efforts that exist. Furthermore, we investigate the state of high performance computing and supercomputing, as well touch upon some of the current Exascale efforts.

Chapter \ref{chap:cloud2011} takes a look at the potential for virtualization, in a base case, to support high performance computing. This includes a feature comparison for hardware availability of a few common hypervisors, specifically Xen, KVM, VirtualBox, and VMWare. Then, a few common HPC benchmarks are evaluated in a single node configuration to determine what overhead exists and where. This identifies how in some scenarios, virtualization adds only a minor overhead, whereas with other scenarios, overheads can be up to 50\% compared to native configurations. 

Chapter \ref{chap:hpgc2014} starts to overcome one of the main limitations of virtualization for use in advanced scientific computing, specifically the lack of hardware availability. In this chapter, The Xen hypervisor is used to demonstrate the effect of GPU passthrough, allowing for GPUs to be used in a guest VM. The efficiency of this method is briefly evaluated using two different hardware setups, and finds hardware can play a notable role in single node performance. 

Chapter \ref{chap:cloud2014} continues where chapter \ref{chap:hpgc2014} leaves off, by demonstrating that GPU passthrough is possible on many other hypervisors, specifically also KVM and VMWare, and compares with one of the main containerization solutions, LXC. Here, the GPUs are evaluated using not only the SHOC GPU benchmark suite developed at Oak Ridge National Laboratory, but also a diverse mix of real-world applications in order to examine how and where overhead exists with GPUs in VMs for each virtualization setup.  Specifically, we find that with properly tuned hardware and NUMA-balanced configurations, the KVM solution can perform at roughly near-native performance, with on average ~1.5\% overhead compared to no virtualization. This illustrates that with the correct hypervisor selection, careful tuning, and advanced hardware, scientific computations can be supported using virtualized hardware. 

Chapter \ref{chap:mdsimulations} takes the findings from the previous chapter to the next level. Specifically, the lessons learned from successful KVM virtualization with GPUs are expanded and combined with a missing key component of supporting advanced parallel computations: a high speed, low latency interconnect, specifically InfiniBand. Using SR-IOV and PCI passthrough of QDR InfiniBand interconnect across a small cluster, it is demonstrated that two Molecular Dynamics simulations, both very commonly used in the HPC community, can be run at near-native performance in the designed virtual cluster.

Chapter \ref{chap:future-work} takes a look at the given situation of virtualization, and puts forth an argument for enhancements forthcoming in high performance virtual cluster solutions. Specifically, we look at the given state of the art, how virtual clusters can be used to provide an infrastructure to support the convergence between HPC and big data. Specifically, this chapter outlines and investigates potential next steps for virtualization, including the potential for advanced live migration techniques and VM cloning, which can be made available with the inclusion of a high-performance RDMA-capable interconnect. 

Finally, this work concludes with an overall view of the current state of high performance virtualization, as well as its potential to impact and support a wide array of disciplines. 



